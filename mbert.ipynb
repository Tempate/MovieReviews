{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOHv8FbPhlv680Vxz7j32MR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tempate/Prophet/blob/main/mbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a classifier with mBERT"
      ],
      "metadata": {
        "id": "_2EWkcNuoI7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by installing the huggingface transformer package."
      ],
      "metadata": {
        "id": "4JOtrsq9oWRK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xd_oAReqncD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16a8060-1010-4689-8032-c6d051bb8800"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data"
      ],
      "metadata": {
        "id": "fhCXdttsog-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the dataset."
      ],
      "metadata": {
        "id": "_OFIzs-No5Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/My Drive/dataset (1).csv') as file:\n",
        "  dataset = file.read().splitlines()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5IbuiYhoTci",
        "outputId": "10c5d32a-7e27-4e72-c5e2-e60956296e44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove characters that aren't letters."
      ],
      "metadata": {
        "id": "P7302eTdo8f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "import random\n",
        "\n",
        "\n",
        "SAMPLE_COUNT = 19\n",
        "\n",
        "data = []\n",
        "\n",
        "for entry in random.sample(dataset, SAMPLE_COUNT):\n",
        "    text = re.findall('\\p{L}+', entry.lower())\n",
        "    data.append((\" \".join(text), int(entry[-1])))\n",
        "\n",
        "# Remove the dataset from memory\n",
        "del dataset"
      ],
      "metadata": {
        "id": "ggfbUV97ogC_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the model"
      ],
      "metadata": {
        "id": "K3YkcoNEpbID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea is to use mBERT as the input vector for the classifier."
      ],
      "metadata": {
        "id": "LbbL-9MApijS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.function1 = nn.Linear(input_size, 16)\n",
        "        self.activation1 = nn.ReLU()\n",
        "\n",
        "        self.function2 = nn.Linear(16, 16)\n",
        "        self.activation2 = nn.ReLU()\n",
        "\n",
        "        self.function3 = nn.Linear(16, 1)\n",
        "        self.activation3 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, vector):\n",
        "        layer1 = self.activation1(self.function1(vector))\n",
        "        layer2 = self.activation2(self.function2(layer1))\n",
        "        layer3 = self.activation3(self.function3(layer2))\n",
        "\n",
        "        return (layer3[0]).float()\n"
      ],
      "metadata": {
        "id": "OFEMmrJfpTw0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are algo going to create a class for our dataset."
      ],
      "metadata": {
        "id": "F1CFOEPDp-Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "  def __init__(self, review, target, tokenizer, max_length):\n",
        "    self.review = review\n",
        "    self.target = target\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.review)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.review[item])\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      \n",
        "      add_special_tokens=True,\n",
        "      \n",
        "      truncation=True,\n",
        "      padding='max_length',\n",
        "      max_length=self.max_length,\n",
        "      \n",
        "      return_token_type_ids=False,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'text': review,\n",
        "      'targets': torch.tensor(self.target[item], dtype=torch.long),\n",
        "\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'input_ids': encoding['input_ids'].flatten()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "KCNCDk6Ep2no"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging the classifier with mBERT."
      ],
      "metadata": {
        "id": "YssN4AgEqUid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class has two purposes: to vectorize data by passing it to mBERT and to prepare data using DataLoader and our ReviewDataset."
      ],
      "metadata": {
        "id": "jof6U-mqqi7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "NUM_WORKERS = 2\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "\n",
        "class Vectorizer():\n",
        "    def __init__(self, MODEL):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
        "        self.model = BertModel.from_pretrained(MODEL)\n",
        "\n",
        "    def vectorize(self, data):\n",
        "        self.vectors = {}\n",
        "\n",
        "        for entry in data:\n",
        "            for batch in self.data_loader([entry]):\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    id = batch[\"text\"][0]\n",
        "                    \n",
        "                    self.vectors[id] = self.model(\n",
        "                        input_ids=batch[\"input_ids\"], \n",
        "                        attention_mask=batch[\"attention_mask\"]\n",
        "                    )[1]\n",
        "\n",
        "        print(\"[+] Data vectorized correctly\")\n",
        "\n",
        "    def data_loader(self, data):\n",
        "        texts, labels = zip(*data)\n",
        "\n",
        "        entry = ReviewDataset(\n",
        "            np.array(texts),\n",
        "            np.array(labels),\n",
        "            self.tokenizer,\n",
        "            MAX_LENGTH\n",
        "        )\n",
        "\n",
        "        loader = DataLoader(\n",
        "            entry, \n",
        "            batch_size=BATCH_SIZE, \n",
        "            num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        return loader\n"
      ],
      "metadata": {
        "id": "RaGn6i-UqJn7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "szeS1VL3rMvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class combines all the previous classes to train and validate our model."
      ],
      "metadata": {
        "id": "oFDcOl6HrZ72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "MODEL = 'bert-base-multilingual-uncased'\n",
        "\n",
        "LEARNING_RATE = 5e-3\n",
        "\n",
        "\n",
        "class Transformers:\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.vectorizer = Vectorizer(MODEL)\n",
        "        self.vectorizer.vectorize(data)\n",
        "\n",
        "        self.model = Classifier(self.vectorizer.model.config.hidden_size)\n",
        "\n",
        "    def train(self, train_data, valid_data, epochs, patience):\n",
        "        optimizer = optim.NAdam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "        loss_function = nn.BCELoss()\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        # We pass several times over the training data.\n",
        "        # Usually there are between 5 and 30 epochs.\n",
        "        for epoch in range(epochs):\n",
        "            for review in train_data:\n",
        "              for batch in self.vectorizer.data_loader([review]):\n",
        "                  \n",
        "                  vector = self.vectorizer.vectors[batch[\"text\"][0]]\n",
        "                  target = batch[\"targets\"].float()\n",
        "\n",
        "                  # We clear the gradients before each instance\n",
        "                  self.model.zero_grad()\n",
        "\n",
        "                  # We run the forward pass\n",
        "                  log_probs = self.model(vector)\n",
        "\n",
        "                  # We compute the loss, gradients, and update the parameters\n",
        "                  loss = loss_function(log_probs, target)\n",
        "                  loss.backward()\n",
        "\n",
        "                  optimizer.step()\n",
        "\n",
        "            score, loss = self.validate(valid_data, loss_function)\n",
        "            losses.append(loss)\n",
        "\n",
        "            print(f\"{epoch + 1}.\\tLoss: {loss}\\tF1-Score: {score}\")\n",
        "\n",
        "            # We count the patience to avoid over-fitting\n",
        "            if len(losses) >= 2 and losses[-1] - losses[-2] >= 0:\n",
        "                patience -= 1\n",
        "\n",
        "            if patience <= 0:\n",
        "                break\n",
        "\n",
        "        return score, loss\n",
        "\n",
        "    def validate(self, data, loss_function):\n",
        "        targets = []\n",
        "        guesses = []\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        batches = self.vectorizer.data_loader(data)\n",
        "        \n",
        "        for batch in batches:\n",
        "\n",
        "            vector = self.vectorizer.vectors[batch[\"text\"][0]]\n",
        "            \n",
        "            target = batch[\"targets\"].float()\n",
        "            targets.append(target)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # We run the forward pass\n",
        "                output = self.model(vector)\n",
        "\n",
        "                # We save our prediction\n",
        "                guess = torch.round(torch.flatten(output))\n",
        "                guesses.append(guess)\n",
        "                \n",
        "                # We calculate the loss\n",
        "                loss += loss_function(output, target).item()\n",
        "\n",
        "        score = f1_score(targets, guesses, zero_division=1)\n",
        "        loss /= len(batches)\n",
        "\n",
        "        return score, loss\n"
      ],
      "metadata": {
        "id": "RFLHbmAvrQTt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing our model"
      ],
      "metadata": {
        "id": "kd7hSVKGrobB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach is good when testing with a tiny dataset. In our case we only have 19 entries."
      ],
      "metadata": {
        "id": "SDnDSVNZrrQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER_OF_TIMES_TO_TEST = 5\n",
        "EPOCHS = 100\n",
        "PATIENCE = 10\n",
        "\n",
        "def leave_one_out(data):\n",
        "    total_score = 0\n",
        "\n",
        "    for n in range(NUMBER_OF_TIMES_TO_TEST):\n",
        "        score = 0\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            train = data[:i] + data[i+1:]\n",
        "            valid = [data[i]]\n",
        "\n",
        "            model = Transformers(data)\n",
        "            score += model.train(train, valid, EPOCHS, PATIENCE)[0]\n",
        "\n",
        "        score /= len(data)\n",
        "        total_score += score\n",
        "\n",
        "        print(\"[%d]\\t%.4f\" % (n + 1, score))\n",
        "\n",
        "    print(\"Average score: %.4f\" % (total_score / NUMBER_OF_TIMES_TO_TEST))\n",
        "\n",
        "\n",
        "leave_one_out(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvM4ZAYfrTAp",
        "outputId": "8bfe83c9-fe42-4711-87a9-cf47db31fd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6190440058708191\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6032919883728027\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5857675671577454\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5696262717247009\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5541063547134399\tF1-Score: 1.0\n",
            "6.\tLoss: 0.542468786239624\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5332370400428772\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5258433222770691\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5199636816978455\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5150814652442932\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5110149383544922\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5076237916946411\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5047928690910339\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5024266839027405\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5004462599754333\tF1-Score: 1.0\n",
            "16.\tLoss: 0.4987863004207611\tF1-Score: 1.0\n",
            "17.\tLoss: 0.4973927140235901\tF1-Score: 1.0\n",
            "18.\tLoss: 0.4962209463119507\tF1-Score: 1.0\n",
            "19.\tLoss: 0.4952346682548523\tF1-Score: 1.0\n",
            "20.\tLoss: 0.494403600692749\tF1-Score: 1.0\n",
            "21.\tLoss: 0.49370276927948\tF1-Score: 1.0\n",
            "22.\tLoss: 0.49311643838882446\tF1-Score: 1.0\n",
            "23.\tLoss: 0.4926508963108063\tF1-Score: 1.0\n",
            "24.\tLoss: 0.49225637316703796\tF1-Score: 1.0\n",
            "25.\tLoss: 0.49192148447036743\tF1-Score: 1.0\n",
            "26.\tLoss: 0.49163755774497986\tF1-Score: 1.0\n",
            "27.\tLoss: 0.491396963596344\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4911934733390808\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49102166295051575\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4908767342567444\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4907548427581787\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49065279960632324\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49056732654571533\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4904964566230774\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4904378652572632\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4903900623321533\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4903514087200165\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4903206527233124\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4902969300746918\tF1-Score: 1.0\n",
            "40.\tLoss: 0.49027901887893677\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4902665615081787\tF1-Score: 1.0\n",
            "42.\tLoss: 0.49025848507881165\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4902544915676117\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49025389552116394\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4902564287185669\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4902614951133728\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49026909470558167\tF1-Score: 1.0\n",
            "48.\tLoss: 0.490278422832489\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4902896285057068\tF1-Score: 1.0\n",
            "50.\tLoss: 0.49030226469039917\tF1-Score: 1.0\n",
            "51.\tLoss: 0.4903161823749542\tF1-Score: 1.0\n",
            "52.\tLoss: 0.49033117294311523\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4903470575809479\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4903636872768402\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6212506294250488\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5083303451538086\tF1-Score: 1.0\n",
            "3.\tLoss: 0.48239952325820923\tF1-Score: 1.0\n",
            "4.\tLoss: 0.48061835765838623\tF1-Score: 1.0\n",
            "5.\tLoss: 0.481477826833725\tF1-Score: 1.0\n",
            "6.\tLoss: 0.48208749294281006\tF1-Score: 1.0\n",
            "7.\tLoss: 0.4824933707714081\tF1-Score: 1.0\n",
            "8.\tLoss: 0.4827989637851715\tF1-Score: 1.0\n",
            "9.\tLoss: 0.48301786184310913\tF1-Score: 1.0\n",
            "10.\tLoss: 0.4831470549106598\tF1-Score: 1.0\n",
            "11.\tLoss: 0.48273220658302307\tF1-Score: 1.0\n",
            "12.\tLoss: 0.48249125480651855\tF1-Score: 1.0\n",
            "13.\tLoss: 0.4823012351989746\tF1-Score: 1.0\n",
            "14.\tLoss: 0.4820607602596283\tF1-Score: 1.0\n",
            "15.\tLoss: 0.48172634840011597\tF1-Score: 1.0\n",
            "16.\tLoss: 0.4817485511302948\tF1-Score: 1.0\n",
            "17.\tLoss: 0.4814371168613434\tF1-Score: 1.0\n",
            "18.\tLoss: 0.4802631735801697\tF1-Score: 1.0\n",
            "19.\tLoss: 0.4793137013912201\tF1-Score: 1.0\n",
            "20.\tLoss: 0.47931215167045593\tF1-Score: 1.0\n",
            "21.\tLoss: 0.4780392646789551\tF1-Score: 1.0\n",
            "22.\tLoss: 0.4768325090408325\tF1-Score: 1.0\n",
            "23.\tLoss: 0.47566232085227966\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4755198061466217\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4738099277019501\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4722338914871216\tF1-Score: 1.0\n",
            "27.\tLoss: 0.47077620029449463\tF1-Score: 1.0\n",
            "28.\tLoss: 0.469277560710907\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4676675796508789\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4659592807292938\tF1-Score: 1.0\n",
            "31.\tLoss: 0.46439993381500244\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4635155200958252\tF1-Score: 1.0\n",
            "33.\tLoss: 0.45460817217826843\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4574851393699646\tF1-Score: 1.0\n",
            "35.\tLoss: 0.5618419647216797\tF1-Score: 1.0\n",
            "36.\tLoss: 0.42769524455070496\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4383344054222107\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 1.059593677520752\tF1-Score: 0.0\n",
            "2.\tLoss: 1.1606991291046143\tF1-Score: 0.0\n",
            "3.\tLoss: 1.1247069835662842\tF1-Score: 0.0\n",
            "4.\tLoss: 1.1170135736465454\tF1-Score: 0.0\n",
            "5.\tLoss: 1.1134294271469116\tF1-Score: 0.0\n",
            "6.\tLoss: 1.1099966764450073\tF1-Score: 0.0\n",
            "7.\tLoss: 1.1082415580749512\tF1-Score: 0.0\n",
            "8.\tLoss: 1.2058563232421875\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0986970663070679\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0987474918365479\tF1-Score: 0.0\n",
            "11.\tLoss: 1.1028931140899658\tF1-Score: 0.0\n",
            "12.\tLoss: 1.1029994487762451\tF1-Score: 0.0\n",
            "13.\tLoss: 1.1090290546417236\tF1-Score: 0.0\n",
            "14.\tLoss: 1.1011141538619995\tF1-Score: 0.0\n",
            "15.\tLoss: 1.1078579425811768\tF1-Score: 0.0\n",
            "16.\tLoss: 1.1076323986053467\tF1-Score: 0.0\n",
            "17.\tLoss: 1.1014184951782227\tF1-Score: 0.0\n",
            "18.\tLoss: 1.100975751876831\tF1-Score: 0.0\n",
            "19.\tLoss: 1.1091190576553345\tF1-Score: 0.0\n",
            "20.\tLoss: 1.10247802734375\tF1-Score: 0.0\n",
            "21.\tLoss: 1.110443353652954\tF1-Score: 0.0\n",
            "22.\tLoss: 1.1034506559371948\tF1-Score: 0.0\n",
            "23.\tLoss: 1.1107606887817383\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 1.119490146636963\tF1-Score: 0.0\n",
            "2.\tLoss: 1.0795447826385498\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0630652904510498\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0631237030029297\tF1-Score: 0.0\n",
            "5.\tLoss: 1.063907504081726\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0643653869628906\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0656185150146484\tF1-Score: 0.0\n",
            "8.\tLoss: 1.067048192024231\tF1-Score: 0.0\n",
            "9.\tLoss: 1.067569375038147\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0696942806243896\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0706816911697388\tF1-Score: 0.0\n",
            "12.\tLoss: 1.0804648399353027\tF1-Score: 0.0\n",
            "13.\tLoss: 1.0861347913742065\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 1.1852777004241943\tF1-Score: 0.0\n",
            "2.\tLoss: 1.234259843826294\tF1-Score: 0.0\n",
            "3.\tLoss: 1.2202656269073486\tF1-Score: 0.0\n",
            "4.\tLoss: 1.213104009628296\tF1-Score: 0.0\n",
            "5.\tLoss: 1.209858775138855\tF1-Score: 0.0\n",
            "6.\tLoss: 1.2074637413024902\tF1-Score: 0.0\n",
            "7.\tLoss: 1.2053370475769043\tF1-Score: 0.0\n",
            "8.\tLoss: 1.2035056352615356\tF1-Score: 0.0\n",
            "9.\tLoss: 1.201987624168396\tF1-Score: 0.0\n",
            "10.\tLoss: 1.20090651512146\tF1-Score: 0.0\n",
            "11.\tLoss: 1.200021743774414\tF1-Score: 0.0\n",
            "12.\tLoss: 1.1992323398590088\tF1-Score: 0.0\n",
            "13.\tLoss: 1.1985770463943481\tF1-Score: 0.0\n",
            "14.\tLoss: 1.1983046531677246\tF1-Score: 0.0\n",
            "15.\tLoss: 1.1977170705795288\tF1-Score: 0.0\n",
            "16.\tLoss: 1.1974787712097168\tF1-Score: 0.0\n",
            "17.\tLoss: 1.1973206996917725\tF1-Score: 0.0\n",
            "18.\tLoss: 1.1972334384918213\tF1-Score: 0.0\n",
            "19.\tLoss: 1.1972534656524658\tF1-Score: 0.0\n",
            "20.\tLoss: 1.1974053382873535\tF1-Score: 0.0\n",
            "21.\tLoss: 1.1975464820861816\tF1-Score: 0.0\n",
            "22.\tLoss: 1.1978708505630493\tF1-Score: 0.0\n",
            "23.\tLoss: 1.1981847286224365\tF1-Score: 0.0\n",
            "24.\tLoss: 1.1986333131790161\tF1-Score: 0.0\n",
            "25.\tLoss: 1.1991068124771118\tF1-Score: 0.0\n",
            "26.\tLoss: 1.1996556520462036\tF1-Score: 0.0\n",
            "27.\tLoss: 1.2002865076065063\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7806882858276367\tF1-Score: 0.0\n",
            "2.\tLoss: 0.7368279099464417\tF1-Score: 0.0\n",
            "3.\tLoss: 0.699065625667572\tF1-Score: 0.0\n",
            "4.\tLoss: 0.6652102470397949\tF1-Score: 1.0\n",
            "5.\tLoss: 0.6355254054069519\tF1-Score: 1.0\n",
            "6.\tLoss: 0.6100040674209595\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5884284973144531\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5704309940338135\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5555670857429504\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5433767437934875\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5334252119064331\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5253244042396545\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5187841653823853\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5134801268577576\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5091602206230164\tF1-Score: 1.0\n",
            "16.\tLoss: 0.505639910697937\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5027687549591064\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5004248023033142\tF1-Score: 1.0\n",
            "19.\tLoss: 0.4985092282295227\tF1-Score: 1.0\n",
            "20.\tLoss: 0.4969422221183777\tF1-Score: 1.0\n",
            "21.\tLoss: 0.49565935134887695\tF1-Score: 1.0\n",
            "22.\tLoss: 0.4946080446243286\tF1-Score: 1.0\n",
            "23.\tLoss: 0.49374592304229736\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4930388331413269\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4924587309360504\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4919830858707428\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49159303307533264\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4912737309932709\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49101290106773376\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4908002018928528\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4906274676322937\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4904877841472626\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4903755486011505\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4902862012386322\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4902159571647644\tF1-Score: 1.0\n",
            "36.\tLoss: 0.490161657333374\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49012038111686707\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49009042978286743\tF1-Score: 1.0\n",
            "39.\tLoss: 0.49006959795951843\tF1-Score: 1.0\n",
            "40.\tLoss: 0.490056574344635\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49004995822906494\tF1-Score: 1.0\n",
            "42.\tLoss: 0.49004876613616943\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4900522828102112\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49005967378616333\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4900701940059662\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4900834262371063\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4900989830493927\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49011629819869995\tF1-Score: 1.0\n",
            "49.\tLoss: 0.49013519287109375\tF1-Score: 1.0\n",
            "50.\tLoss: 0.490155428647995\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49017664790153503\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4901988208293915\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7195601463317871\tF1-Score: 0.0\n",
            "2.\tLoss: 0.9848086833953857\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0822395086288452\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0934683084487915\tF1-Score: 0.0\n",
            "5.\tLoss: 1.0914844274520874\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0901715755462646\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0898025035858154\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0897811651229858\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0898922681808472\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0900721549987793\tF1-Score: 0.0\n",
            "11.\tLoss: 1.090285301208496\tF1-Score: 0.0\n",
            "12.\tLoss: 1.0905060768127441\tF1-Score: 0.0\n",
            "13.\tLoss: 1.0907232761383057\tF1-Score: 0.0\n",
            "14.\tLoss: 1.0909380912780762\tF1-Score: 0.0\n",
            "15.\tLoss: 1.0911571979522705\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6078998446464539\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5848588347434998\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5656788349151611\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5501943230628967\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5378090143203735\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5279470086097717\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5201225876808167\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5139334201812744\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5090491771697998\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5052000284194946\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5021693110466003\tF1-Score: 1.0\n",
            "12.\tLoss: 0.49978283047676086\tF1-Score: 1.0\n",
            "13.\tLoss: 0.49790331721305847\tF1-Score: 1.0\n",
            "14.\tLoss: 0.49642205238342285\tF1-Score: 1.0\n",
            "15.\tLoss: 0.4952540397644043\tF1-Score: 1.0\n",
            "16.\tLoss: 0.4943326413631439\tF1-Score: 1.0\n",
            "17.\tLoss: 0.49360552430152893\tF1-Score: 1.0\n",
            "18.\tLoss: 0.4930320978164673\tF1-Score: 1.0\n",
            "19.\tLoss: 0.492580384016037\tF1-Score: 1.0\n",
            "20.\tLoss: 0.49222517013549805\tF1-Score: 1.0\n",
            "21.\tLoss: 0.49194711446762085\tF1-Score: 1.0\n",
            "22.\tLoss: 0.49173033237457275\tF1-Score: 1.0\n",
            "23.\tLoss: 0.4915629029273987\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4914352595806122\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4913395941257477\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4912700355052948\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49122172594070435\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49119073152542114\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49117419123649597\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4911693036556244\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49117419123649597\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49118712544441223\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49120691418647766\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49123212695121765\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4912620484828949\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4912956655025482\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49133238196372986\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4913715422153473\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4914126694202423\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4914553463459015\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7733896374702454\tF1-Score: 0.0\n",
            "2.\tLoss: 0.7476636171340942\tF1-Score: 0.0\n",
            "3.\tLoss: 0.7197152972221375\tF1-Score: 0.0\n",
            "4.\tLoss: 0.6959806084632874\tF1-Score: 0.0\n",
            "5.\tLoss: 0.6738236546516418\tF1-Score: 1.0\n",
            "6.\tLoss: 0.6527543663978577\tF1-Score: 1.0\n",
            "7.\tLoss: 0.6328756213188171\tF1-Score: 1.0\n",
            "8.\tLoss: 0.6143747568130493\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5975285172462463\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5824818015098572\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5691022276878357\tF1-Score: 1.0\n",
            "12.\tLoss: 0.557378351688385\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5472307801246643\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5385333895683289\tF1-Score: 1.0\n",
            "15.\tLoss: 0.531135082244873\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5248762965202332\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5196020007133484\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5151686668395996\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5114479660987854\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5083276033401489\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5057113170623779\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5035169124603271\tF1-Score: 1.0\n",
            "23.\tLoss: 0.501675546169281\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5001289248466492\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4988288879394531\tF1-Score: 1.0\n",
            "26.\tLoss: 0.49773484468460083\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49681341648101807\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4960363209247589\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4953802227973938\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4948257505893707\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49435660243034363\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49395933747291565\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49362269043922424\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4933370351791382\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4930945634841919\tF1-Score: 1.0\n",
            "36.\tLoss: 0.492888480424881\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49271345138549805\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4925645887851715\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4924379587173462\tF1-Score: 1.0\n",
            "40.\tLoss: 0.492330402135849\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4922388195991516\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4921611249446869\tF1-Score: 1.0\n",
            "43.\tLoss: 0.492095023393631\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4920389652252197\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4919915795326233\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4919514060020447\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4919174909591675\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49188902974128723\tF1-Score: 1.0\n",
            "49.\tLoss: 0.49186524748802185\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4918454587459564\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49182915687561035\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4918157160282135\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4918048083782196\tF1-Score: 1.0\n",
            "54.\tLoss: 0.49179622530937195\tF1-Score: 1.0\n",
            "55.\tLoss: 0.49178948998451233\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4917844235897064\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4917808175086975\tF1-Score: 1.0\n",
            "58.\tLoss: 0.49177849292755127\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4917772114276886\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49177682399749756\tF1-Score: 1.0\n",
            "61.\tLoss: 0.49177733063697815\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49177849292755127\tF1-Score: 1.0\n",
            "63.\tLoss: 0.49178043007850647\tF1-Score: 1.0\n",
            "64.\tLoss: 0.49178266525268555\tF1-Score: 1.0\n",
            "65.\tLoss: 0.4917854964733124\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4917888045310974\tF1-Score: 1.0\n",
            "67.\tLoss: 0.49179232120513916\tF1-Score: 1.0\n",
            "68.\tLoss: 0.4917963147163391\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4918005168437958\tF1-Score: 1.0\n",
            "70.\tLoss: 0.4918048083782196\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 1.2808622121810913\tF1-Score: 0.0\n",
            "2.\tLoss: 1.2304704189300537\tF1-Score: 0.0\n",
            "3.\tLoss: 1.1923195123672485\tF1-Score: 0.0\n",
            "4.\tLoss: 1.1811188459396362\tF1-Score: 0.0\n",
            "5.\tLoss: 1.1813764572143555\tF1-Score: 0.0\n",
            "6.\tLoss: 1.1701995134353638\tF1-Score: 0.0\n",
            "7.\tLoss: 1.1602649688720703\tF1-Score: 0.0\n",
            "8.\tLoss: 1.152870535850525\tF1-Score: 0.0\n",
            "9.\tLoss: 1.1476305723190308\tF1-Score: 0.0\n",
            "10.\tLoss: 1.142870306968689\tF1-Score: 0.0\n",
            "11.\tLoss: 1.138620138168335\tF1-Score: 0.0\n",
            "12.\tLoss: 1.1404668092727661\tF1-Score: 0.0\n",
            "13.\tLoss: 1.1372631788253784\tF1-Score: 0.0\n",
            "14.\tLoss: 1.1307717561721802\tF1-Score: 0.0\n",
            "15.\tLoss: 1.1275876760482788\tF1-Score: 0.0\n",
            "16.\tLoss: 1.1255704164505005\tF1-Score: 0.0\n",
            "17.\tLoss: 1.1237351894378662\tF1-Score: 0.0\n",
            "18.\tLoss: 1.1219666004180908\tF1-Score: 0.0\n",
            "19.\tLoss: 1.1203020811080933\tF1-Score: 0.0\n",
            "20.\tLoss: 1.1187479496002197\tF1-Score: 0.0\n",
            "21.\tLoss: 1.1189357042312622\tF1-Score: 0.0\n",
            "22.\tLoss: 1.116318702697754\tF1-Score: 0.0\n",
            "23.\tLoss: 1.11464262008667\tF1-Score: 0.0\n",
            "24.\tLoss: 1.113674521446228\tF1-Score: 0.0\n",
            "25.\tLoss: 1.1124521493911743\tF1-Score: 0.0\n",
            "26.\tLoss: 1.1117780208587646\tF1-Score: 0.0\n",
            "27.\tLoss: 1.1104860305786133\tF1-Score: 0.0\n",
            "28.\tLoss: 1.1101497411727905\tF1-Score: 0.0\n",
            "29.\tLoss: 1.11089026927948\tF1-Score: 0.0\n",
            "30.\tLoss: 1.1092137098312378\tF1-Score: 0.0\n",
            "31.\tLoss: 1.108127474784851\tF1-Score: 0.0\n",
            "32.\tLoss: 1.10726797580719\tF1-Score: 0.0\n",
            "33.\tLoss: 1.1065642833709717\tF1-Score: 0.0\n",
            "34.\tLoss: 1.1078888177871704\tF1-Score: 0.0\n",
            "35.\tLoss: 1.105933427810669\tF1-Score: 0.0\n",
            "36.\tLoss: 1.1048901081085205\tF1-Score: 0.0\n",
            "37.\tLoss: 1.10434889793396\tF1-Score: 0.0\n",
            "38.\tLoss: 1.1022911071777344\tF1-Score: 0.0\n",
            "39.\tLoss: 1.1050755977630615\tF1-Score: 0.0\n",
            "40.\tLoss: 1.1020209789276123\tF1-Score: 0.0\n",
            "41.\tLoss: 1.0962215662002563\tF1-Score: 0.0\n",
            "42.\tLoss: 1.100921392440796\tF1-Score: 0.0\n",
            "43.\tLoss: 1.1042207479476929\tF1-Score: 0.0\n",
            "44.\tLoss: 1.0998663902282715\tF1-Score: 0.0\n",
            "45.\tLoss: 1.100960373878479\tF1-Score: 0.0\n",
            "46.\tLoss: 1.098610520362854\tF1-Score: 0.0\n",
            "47.\tLoss: 1.1020722389221191\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.9867160320281982\tF1-Score: 0.0\n",
            "2.\tLoss: 1.0502163171768188\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0807082653045654\tF1-Score: 0.0\n",
            "4.\tLoss: 1.089363694190979\tF1-Score: 0.0\n",
            "5.\tLoss: 1.092118263244629\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0909878015518188\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0898410081863403\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0889955759048462\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0882867574691772\tF1-Score: 0.0\n",
            "10.\tLoss: 1.087626338005066\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0869929790496826\tF1-Score: 0.0\n",
            "12.\tLoss: 1.0863877534866333\tF1-Score: 0.0\n",
            "13.\tLoss: 1.0858138799667358\tF1-Score: 0.0\n",
            "14.\tLoss: 1.0852733850479126\tF1-Score: 0.0\n",
            "15.\tLoss: 1.0847665071487427\tF1-Score: 0.0\n",
            "16.\tLoss: 1.0842927694320679\tF1-Score: 0.0\n",
            "17.\tLoss: 1.0838508605957031\tF1-Score: 0.0\n",
            "18.\tLoss: 1.0834389925003052\tF1-Score: 0.0\n",
            "19.\tLoss: 1.0830557346343994\tF1-Score: 0.0\n",
            "20.\tLoss: 1.0826995372772217\tF1-Score: 0.0\n",
            "21.\tLoss: 1.0823687314987183\tF1-Score: 0.0\n",
            "22.\tLoss: 1.082061767578125\tF1-Score: 0.0\n",
            "23.\tLoss: 1.0817774534225464\tF1-Score: 0.0\n",
            "24.\tLoss: 1.0815141201019287\tF1-Score: 0.0\n",
            "25.\tLoss: 1.081270456314087\tF1-Score: 0.0\n",
            "26.\tLoss: 1.0810449123382568\tF1-Score: 0.0\n",
            "27.\tLoss: 1.0808366537094116\tF1-Score: 0.0\n",
            "28.\tLoss: 1.080643892288208\tF1-Score: 0.0\n",
            "29.\tLoss: 1.0804659128189087\tF1-Score: 0.0\n",
            "30.\tLoss: 1.08030104637146\tF1-Score: 0.0\n",
            "31.\tLoss: 1.0801481008529663\tF1-Score: 0.0\n",
            "32.\tLoss: 1.0800060033798218\tF1-Score: 0.0\n",
            "33.\tLoss: 1.0836429595947266\tF1-Score: 0.0\n",
            "34.\tLoss: 1.081688404083252\tF1-Score: 0.0\n",
            "35.\tLoss: 1.0798982381820679\tF1-Score: 0.0\n",
            "36.\tLoss: 1.0789579153060913\tF1-Score: 0.0\n",
            "37.\tLoss: 1.078541874885559\tF1-Score: 0.0\n",
            "38.\tLoss: 1.0783662796020508\tF1-Score: 0.0\n",
            "39.\tLoss: 1.078285574913025\tF1-Score: 0.0\n",
            "40.\tLoss: 1.0782365798950195\tF1-Score: 0.0\n",
            "41.\tLoss: 1.0781941413879395\tF1-Score: 0.0\n",
            "42.\tLoss: 1.0781501531600952\tF1-Score: 0.0\n",
            "43.\tLoss: 1.0781004428863525\tF1-Score: 0.0\n",
            "44.\tLoss: 1.078043818473816\tF1-Score: 0.0\n",
            "45.\tLoss: 1.0779794454574585\tF1-Score: 0.0\n",
            "46.\tLoss: 1.0779067277908325\tF1-Score: 0.0\n",
            "47.\tLoss: 1.0778254270553589\tF1-Score: 0.0\n",
            "48.\tLoss: 1.0777348279953003\tF1-Score: 0.0\n",
            "49.\tLoss: 1.0776344537734985\tF1-Score: 0.0\n",
            "50.\tLoss: 1.0775238275527954\tF1-Score: 0.0\n",
            "51.\tLoss: 1.095389723777771\tF1-Score: 0.0\n",
            "52.\tLoss: 1.0870001316070557\tF1-Score: 0.0\n",
            "53.\tLoss: 1.0796643495559692\tF1-Score: 0.0\n",
            "54.\tLoss: 1.0764778852462769\tF1-Score: 0.0\n",
            "55.\tLoss: 1.0754650831222534\tF1-Score: 0.0\n",
            "56.\tLoss: 1.075227975845337\tF1-Score: 0.0\n",
            "57.\tLoss: 1.0751783847808838\tF1-Score: 0.0\n",
            "58.\tLoss: 1.0751320123672485\tF1-Score: 0.0\n",
            "59.\tLoss: 1.1152913570404053\tF1-Score: 0.0\n",
            "60.\tLoss: 1.0824559926986694\tF1-Score: 0.0\n",
            "61.\tLoss: 1.0807691812515259\tF1-Score: 0.0\n",
            "62.\tLoss: 1.094826579093933\tF1-Score: 0.0\n",
            "63.\tLoss: 1.0846318006515503\tF1-Score: 0.0\n",
            "64.\tLoss: 1.0766189098358154\tF1-Score: 0.0\n",
            "65.\tLoss: 1.0953421592712402\tF1-Score: 0.0\n",
            "66.\tLoss: 1.0803442001342773\tF1-Score: 0.0\n",
            "67.\tLoss: 1.1089376211166382\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6329155564308167\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6438727378845215\tF1-Score: 1.0\n",
            "3.\tLoss: 0.6188392639160156\tF1-Score: 1.0\n",
            "4.\tLoss: 0.6109501123428345\tF1-Score: 1.0\n",
            "5.\tLoss: 0.594868004322052\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5807433128356934\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5683759450912476\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5575895309448242\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5482246279716492\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5401304364204407\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5331645011901855\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5271919369697571\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5220872759819031\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5177361369132996\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5140346884727478\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5108909606933594\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5082233548164368\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5059611797332764\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5040428638458252\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5024157762527466\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5010347366333008\tF1-Score: 1.0\n",
            "22.\tLoss: 0.49986162781715393\tF1-Score: 1.0\n",
            "23.\tLoss: 0.4988640248775482\tF1-Score: 1.0\n",
            "24.\tLoss: 0.49801450967788696\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4972899854183197\tF1-Score: 1.0\n",
            "26.\tLoss: 0.49667108058929443\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4961414337158203\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4956875145435333\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4952975809574127\tF1-Score: 1.0\n",
            "30.\tLoss: 0.49496203660964966\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4946727454662323\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49442315101623535\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4942069947719574\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4940198063850403\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4938572645187378\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4937160611152649\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4935930073261261\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49348580837249756\tF1-Score: 1.0\n",
            "39.\tLoss: 0.49339237809181213\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4933106601238251\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4932391047477722\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4931766390800476\tF1-Score: 1.0\n",
            "43.\tLoss: 0.49312180280685425\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4930736720561981\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4930315315723419\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4929944574832916\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49296194314956665\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49293336272239685\tF1-Score: 1.0\n",
            "49.\tLoss: 0.49290817975997925\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4928860366344452\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49286651611328125\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4928494393825531\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4928344190120697\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4928213357925415\tF1-Score: 1.0\n",
            "55.\tLoss: 0.49280983209609985\tF1-Score: 1.0\n",
            "56.\tLoss: 0.49279987812042236\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4927912950515747\tF1-Score: 1.0\n",
            "58.\tLoss: 0.49278387427330017\tF1-Score: 1.0\n",
            "59.\tLoss: 0.49277764558792114\tF1-Score: 1.0\n",
            "60.\tLoss: 0.4927723705768585\tF1-Score: 1.0\n",
            "61.\tLoss: 0.49276795983314514\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49276435375213623\tF1-Score: 1.0\n",
            "63.\tLoss: 0.4927618205547333\tF1-Score: 1.0\n",
            "64.\tLoss: 0.49275967478752136\tF1-Score: 1.0\n",
            "65.\tLoss: 0.4927584230899811\tF1-Score: 1.0\n",
            "66.\tLoss: 0.49275752902030945\tF1-Score: 1.0\n",
            "67.\tLoss: 0.4927573502063751\tF1-Score: 1.0\n",
            "68.\tLoss: 0.49275773763656616\tF1-Score: 1.0\n",
            "69.\tLoss: 0.49275851249694824\tF1-Score: 1.0\n",
            "70.\tLoss: 0.4927597641944885\tF1-Score: 1.0\n",
            "71.\tLoss: 0.49276143312454224\tF1-Score: 1.0\n",
            "72.\tLoss: 0.492763489484787\tF1-Score: 1.0\n",
            "73.\tLoss: 0.4927658140659332\tF1-Score: 1.0\n",
            "74.\tLoss: 0.4927685558795929\tF1-Score: 1.0\n",
            "75.\tLoss: 0.49277159571647644\tF1-Score: 1.0\n",
            "76.\tLoss: 0.49277469515800476\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6926242113113403\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5805683135986328\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5386560559272766\tF1-Score: 1.0\n",
            "4.\tLoss: 0.537747859954834\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5999608039855957\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5484752058982849\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5273371338844299\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5211060047149658\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5190329551696777\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5178297758102417\tF1-Score: 1.0\n",
            "11.\tLoss: 0.516840934753418\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5159715414047241\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5151962041854858\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5144920349121094\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5138385891914368\tF1-Score: 1.0\n",
            "16.\tLoss: 0.513220489025116\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5126268863677979\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5120500326156616\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5114844441413879\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5109255313873291\tF1-Score: 1.0\n",
            "21.\tLoss: 0.510370135307312\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5098150968551636\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5094919204711914\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5084888339042664\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5081040859222412\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5073935985565186\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5063353180885315\tF1-Score: 1.0\n",
            "28.\tLoss: 0.508818507194519\tF1-Score: 1.0\n",
            "29.\tLoss: 0.50069659948349\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5014171004295349\tF1-Score: 1.0\n",
            "31.\tLoss: 0.5033562779426575\tF1-Score: 1.0\n",
            "32.\tLoss: 0.5044667720794678\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4957370460033417\tF1-Score: 1.0\n",
            "34.\tLoss: 0.5000811815261841\tF1-Score: 1.0\n",
            "35.\tLoss: 0.5004786849021912\tF1-Score: 1.0\n",
            "36.\tLoss: 0.5018031001091003\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49984660744667053\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4987497627735138\tF1-Score: 1.0\n",
            "39.\tLoss: 0.478019654750824\tF1-Score: 1.0\n",
            "40.\tLoss: 0.47800368070602417\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4933052062988281\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4846426248550415\tF1-Score: 1.0\n",
            "43.\tLoss: 0.47387272119522095\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4872029721736908\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.5384238958358765\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5320359468460083\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5263504981994629\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5233837366104126\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5226306319236755\tF1-Score: 1.0\n",
            "6.\tLoss: 0.522712767124176\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5230034589767456\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5232959985733032\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5235204696655273\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5236433148384094\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5219243168830872\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5205708742141724\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5197998881340027\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5191365480422974\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5140162110328674\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5159494876861572\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5147928595542908\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5129404067993164\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5124520063400269\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5098947882652283\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5067028999328613\tF1-Score: 1.0\n",
            "22.\tLoss: 0.509020984172821\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5064892172813416\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5057193040847778\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5043313503265381\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5038959980010986\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49710115790367126\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5002464056015015\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4963292181491852\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4948059022426605\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49760758876800537\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49460452795028687\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4946324825286865\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.536858856678009\tF1-Score: 1.0\n",
            "2.\tLoss: 0.3583829700946808\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5113580226898193\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5353361368179321\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5377224683761597\tF1-Score: 1.0\n",
            "6.\tLoss: 0.537451446056366\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5373520851135254\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5373882055282593\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5373116731643677\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5369946956634521\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5363818407058716\tF1-Score: 1.0\n",
            "12.\tLoss: 0.535440981388092\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5336708426475525\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5290609002113342\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5231992602348328\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5063266158103943\tF1-Score: 1.0\n",
            "17.\tLoss: 0.514647364616394\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5182891488075256\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5140293836593628\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5157413482666016\tF1-Score: 1.0\n",
            "21.\tLoss: 0.48552918434143066\tF1-Score: 1.0\n",
            "22.\tLoss: 0.48046061396598816\tF1-Score: 1.0\n",
            "23.\tLoss: 0.48398450016975403\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5036071538925171\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4962037205696106\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5074354410171509\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6757416725158691\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6587401628494263\tF1-Score: 1.0\n",
            "3.\tLoss: 0.6424775719642639\tF1-Score: 1.0\n",
            "4.\tLoss: 0.6275922060012817\tF1-Score: 1.0\n",
            "5.\tLoss: 0.6139497756958008\tF1-Score: 1.0\n",
            "6.\tLoss: 0.6013804078102112\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5897753834724426\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5790761709213257\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5692753791809082\tF1-Score: 1.0\n",
            "10.\tLoss: 0.560649037361145\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5527797341346741\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5456520318984985\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5392430424690247\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5335187315940857\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5284363627433777\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5239466428756714\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5199976563453674\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5165367722511292\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5135123133659363\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5108516216278076\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5086008310317993\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5066385865211487\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5049295425415039\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5034418702125549\tF1-Score: 1.0\n",
            "25.\tLoss: 0.502147376537323\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5010213255882263\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5000418424606323\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49918997287750244\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4984486997127533\tF1-Score: 1.0\n",
            "30.\tLoss: 0.49780356884002686\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4972419738769531\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49675267934799194\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4963264763355255\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49595487117767334\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49563068151474\tF1-Score: 1.0\n",
            "36.\tLoss: 0.49534785747528076\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49510079622268677\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49488499760627747\tF1-Score: 1.0\n",
            "39.\tLoss: 0.49469640851020813\tF1-Score: 1.0\n",
            "40.\tLoss: 0.494531512260437\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49438726902008057\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4942609369754791\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4941505193710327\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49405360221862793\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49396881461143494\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4938945770263672\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4938294291496277\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4937722980976105\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4937222898006439\tF1-Score: 1.0\n",
            "50.\tLoss: 0.49367856979370117\tF1-Score: 1.0\n",
            "51.\tLoss: 0.493640273809433\tF1-Score: 1.0\n",
            "52.\tLoss: 0.49360668659210205\tF1-Score: 1.0\n",
            "53.\tLoss: 0.49357739090919495\tF1-Score: 1.0\n",
            "54.\tLoss: 0.49355170130729675\tF1-Score: 1.0\n",
            "55.\tLoss: 0.49352943897247314\tF1-Score: 1.0\n",
            "56.\tLoss: 0.49351000785827637\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4934930205345154\tF1-Score: 1.0\n",
            "58.\tLoss: 0.493478387594223\tF1-Score: 1.0\n",
            "59.\tLoss: 0.49346569180488586\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49345487356185913\tF1-Score: 1.0\n",
            "61.\tLoss: 0.49344557523727417\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49343758821487427\tF1-Score: 1.0\n",
            "63.\tLoss: 0.4934309422969818\tF1-Score: 1.0\n",
            "64.\tLoss: 0.49342527985572815\tF1-Score: 1.0\n",
            "65.\tLoss: 0.4934207797050476\tF1-Score: 1.0\n",
            "66.\tLoss: 0.493416965007782\tF1-Score: 1.0\n",
            "67.\tLoss: 0.493414044380188\tF1-Score: 1.0\n",
            "68.\tLoss: 0.4934118092060089\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4934101402759552\tF1-Score: 1.0\n",
            "70.\tLoss: 0.49340906739234924\tF1-Score: 1.0\n",
            "71.\tLoss: 0.4934084713459015\tF1-Score: 1.0\n",
            "72.\tLoss: 0.49340829253196716\tF1-Score: 1.0\n",
            "73.\tLoss: 0.49340859055519104\tF1-Score: 1.0\n",
            "74.\tLoss: 0.49340927600860596\tF1-Score: 1.0\n",
            "75.\tLoss: 0.49341022968292236\tF1-Score: 1.0\n",
            "76.\tLoss: 0.49341142177581787\tF1-Score: 1.0\n",
            "77.\tLoss: 0.49341297149658203\tF1-Score: 1.0\n",
            "78.\tLoss: 0.4934147298336029\tF1-Score: 1.0\n",
            "79.\tLoss: 0.4934168756008148\tF1-Score: 1.0\n",
            "80.\tLoss: 0.49341902136802673\tF1-Score: 1.0\n",
            "81.\tLoss: 0.49342137575149536\tF1-Score: 1.0\n",
            "82.\tLoss: 0.4934239089488983\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.8104305863380432\tF1-Score: 0.0\n",
            "2.\tLoss: 0.8180322647094727\tF1-Score: 0.0\n",
            "3.\tLoss: 0.8504951596260071\tF1-Score: 0.0\n",
            "4.\tLoss: 0.8808003067970276\tF1-Score: 0.0\n",
            "5.\tLoss: 0.9095478057861328\tF1-Score: 0.0\n",
            "6.\tLoss: 0.9364197254180908\tF1-Score: 0.0\n",
            "7.\tLoss: 0.9610282182693481\tF1-Score: 0.0\n",
            "8.\tLoss: 0.9830806851387024\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0024499893188477\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0191725492477417\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0334099531173706\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6763008832931519\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6614552140235901\tF1-Score: 1.0\n",
            "3.\tLoss: 0.6454972624778748\tF1-Score: 1.0\n",
            "4.\tLoss: 0.629649817943573\tF1-Score: 1.0\n",
            "5.\tLoss: 0.6143364310264587\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5998371243476868\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5863649845123291\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5740622878074646\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5629979968070984\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5531771183013916\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5445534586906433\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5370469093322754\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5305576920509338\tF1-Score: 1.0\n",
            "14.\tLoss: 0.524977445602417\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5201981067657471\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5161166787147522\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5126383900642395\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5096782445907593\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5071610808372498\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5050216913223267\tF1-Score: 1.0\n",
            "21.\tLoss: 0.503203272819519\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5016576051712036\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5003432035446167\tF1-Score: 1.0\n",
            "24.\tLoss: 0.49922510981559753\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4982732832431793\tF1-Score: 1.0\n",
            "26.\tLoss: 0.49746251106262207\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4967713952064514\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4961818754673004\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4956786036491394\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4952486753463745\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4948809742927551\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49456650018692017\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4942972660064697\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49406659603118896\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49386879801750183\tF1-Score: 1.0\n",
            "36.\tLoss: 0.49369916319847107\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4935535788536072\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4934285879135132\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4933212995529175\tF1-Score: 1.0\n",
            "40.\tLoss: 0.49322906136512756\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49314990639686584\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4930818974971771\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4930233359336853\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4929731786251068\tF1-Score: 1.0\n",
            "45.\tLoss: 0.492930144071579\tF1-Score: 1.0\n",
            "46.\tLoss: 0.49289315938949585\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4928615391254425\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49283450841903687\tF1-Score: 1.0\n",
            "49.\tLoss: 0.492811381816864\tF1-Score: 1.0\n",
            "50.\tLoss: 0.49279189109802246\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49277520179748535\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4927610456943512\tF1-Score: 1.0\n",
            "53.\tLoss: 0.49274924397468567\tF1-Score: 1.0\n",
            "54.\tLoss: 0.49273937940597534\tF1-Score: 1.0\n",
            "55.\tLoss: 0.4927311837673187\tF1-Score: 1.0\n",
            "56.\tLoss: 0.49272456765174866\tF1-Score: 1.0\n",
            "57.\tLoss: 0.49271929264068604\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4927150011062622\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4927116930484772\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49270933866500854\tF1-Score: 1.0\n",
            "61.\tLoss: 0.4927077889442444\tF1-Score: 1.0\n",
            "62.\tLoss: 0.4927067160606384\tF1-Score: 1.0\n",
            "63.\tLoss: 0.49270641803741455\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4927065968513489\tF1-Score: 1.0\n",
            "65.\tLoss: 0.49270719289779663\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4927081763744354\tF1-Score: 1.0\n",
            "67.\tLoss: 0.49270954728126526\tF1-Score: 1.0\n",
            "68.\tLoss: 0.49271127581596375\tF1-Score: 1.0\n",
            "69.\tLoss: 0.49271324276924133\tF1-Score: 1.0\n",
            "70.\tLoss: 0.49271559715270996\tF1-Score: 1.0\n",
            "71.\tLoss: 0.4927179217338562\tF1-Score: 1.0\n",
            "72.\tLoss: 0.49272066354751587\tF1-Score: 1.0\n",
            "73.\tLoss: 0.49272358417510986\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6213222742080688\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6008532643318176\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5739682912826538\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5590644478797913\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5493889451026917\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5437943935394287\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5399001240730286\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5367963910102844\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5341479182243347\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5320068001747131\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5298838019371033\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5279141664505005\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5261229872703552\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5244774222373962\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5229482650756836\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5215147137641907\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5201054215431213\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5187103152275085\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5176761150360107\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5165876150131226\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5152949690818787\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5143185257911682\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5132827162742615\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5115188360214233\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5107979774475098\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5100093483924866\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5084227323532104\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5077580809593201\tF1-Score: 1.0\n",
            "29.\tLoss: 0.5062963962554932\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5057035088539124\tF1-Score: 1.0\n",
            "31.\tLoss: 0.5042135119438171\tF1-Score: 1.0\n",
            "32.\tLoss: 0.5035913586616516\tF1-Score: 1.0\n",
            "33.\tLoss: 0.5024335980415344\tF1-Score: 1.0\n",
            "34.\tLoss: 0.5016831159591675\tF1-Score: 1.0\n",
            "35.\tLoss: 0.5022366046905518\tF1-Score: 1.0\n",
            "36.\tLoss: 0.5011643171310425\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4985572397708893\tF1-Score: 1.0\n",
            "38.\tLoss: 0.5016997456550598\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4867817759513855\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4876592755317688\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4893498420715332\tF1-Score: 1.0\n",
            "42.\tLoss: 0.499679297208786\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4970448315143585\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4950129985809326\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49299249053001404\tF1-Score: 1.0\n",
            "46.\tLoss: 0.49146831035614014\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49257373809814453\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49099066853523254\tF1-Score: 1.0\n",
            "49.\tLoss: 0.49020496010780334\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4701993465423584\tF1-Score: 1.0\n",
            "51.\tLoss: 0.4775746464729309\tF1-Score: 1.0\n",
            "52.\tLoss: 0.5466204285621643\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4540809094905853\tF1-Score: 1.0\n",
            "54.\tLoss: 0.45438244938850403\tF1-Score: 1.0\n",
            "55.\tLoss: 0.46382027864456177\tF1-Score: 1.0\n",
            "[1]\t0.6316\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.48258742690086365\tF1-Score: 1.0\n",
            "2.\tLoss: 0.48115238547325134\tF1-Score: 1.0\n",
            "3.\tLoss: 0.49554702639579773\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5009101033210754\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5021902918815613\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5023582577705383\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5022519826889038\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5020281076431274\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5016946196556091\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5012602210044861\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5006856918334961\tF1-Score: 1.0\n",
            "12.\tLoss: 0.500078022480011\tF1-Score: 1.0\n",
            "13.\tLoss: 0.4994048476219177\tF1-Score: 1.0\n",
            "14.\tLoss: 0.49888426065444946\tF1-Score: 1.0\n",
            "15.\tLoss: 0.4981716573238373\tF1-Score: 1.0\n",
            "16.\tLoss: 0.4973415434360504\tF1-Score: 1.0\n",
            "17.\tLoss: 0.496428906917572\tF1-Score: 1.0\n",
            "18.\tLoss: 0.49544546008110046\tF1-Score: 1.0\n",
            "19.\tLoss: 0.4943934381008148\tF1-Score: 1.0\n",
            "20.\tLoss: 0.4932718276977539\tF1-Score: 1.0\n",
            "21.\tLoss: 0.4813823103904724\tF1-Score: 1.0\n",
            "22.\tLoss: 0.48774293065071106\tF1-Score: 1.0\n",
            "23.\tLoss: 0.48953548073768616\tF1-Score: 1.0\n",
            "24.\tLoss: 0.48528552055358887\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4829670488834381\tF1-Score: 1.0\n",
            "26.\tLoss: 0.475749671459198\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4776816964149475\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4786226749420166\tF1-Score: 1.0\n",
            "29.\tLoss: 0.47827771306037903\tF1-Score: 1.0\n",
            "30.\tLoss: 0.47918450832366943\tF1-Score: 1.0\n",
            "31.\tLoss: 0.47816693782806396\tF1-Score: 1.0\n",
            "32.\tLoss: 0.30846458673477173\tF1-Score: 1.0\n",
            "33.\tLoss: 0.23182910680770874\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49851110577583313\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7161861062049866\tF1-Score: 0.0\n",
            "2.\tLoss: 0.6240310668945312\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5573776960372925\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5243604779243469\tF1-Score: 1.0\n",
            "5.\tLoss: 0.513725996017456\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5086737871170044\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5055197477340698\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5033363699913025\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5017808675765991\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5006521344184875\tF1-Score: 1.0\n",
            "11.\tLoss: 0.4996947944164276\tF1-Score: 1.0\n",
            "12.\tLoss: 0.4988867938518524\tF1-Score: 1.0\n",
            "13.\tLoss: 0.4981301724910736\tF1-Score: 1.0\n",
            "14.\tLoss: 0.4973703622817993\tF1-Score: 1.0\n",
            "15.\tLoss: 0.49663376808166504\tF1-Score: 1.0\n",
            "16.\tLoss: 0.4958978295326233\tF1-Score: 1.0\n",
            "17.\tLoss: 0.49514734745025635\tF1-Score: 1.0\n",
            "18.\tLoss: 0.494382381439209\tF1-Score: 1.0\n",
            "19.\tLoss: 0.49359947443008423\tF1-Score: 1.0\n",
            "20.\tLoss: 0.49279069900512695\tF1-Score: 1.0\n",
            "21.\tLoss: 0.49194398522377014\tF1-Score: 1.0\n",
            "22.\tLoss: 0.4911116361618042\tF1-Score: 1.0\n",
            "23.\tLoss: 0.49019327759742737\tF1-Score: 1.0\n",
            "24.\tLoss: 0.48924463987350464\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4882216155529022\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4871244728565216\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4861361086368561\tF1-Score: 1.0\n",
            "28.\tLoss: 0.484859824180603\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4837295114994049\tF1-Score: 1.0\n",
            "30.\tLoss: 0.48250091075897217\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4806777536869049\tF1-Score: 1.0\n",
            "32.\tLoss: 0.47864970564842224\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4774392545223236\tF1-Score: 1.0\n",
            "34.\tLoss: 0.47605156898498535\tF1-Score: 1.0\n",
            "35.\tLoss: 0.47631239891052246\tF1-Score: 1.0\n",
            "36.\tLoss: 0.47328057885169983\tF1-Score: 1.0\n",
            "37.\tLoss: 0.47276297211647034\tF1-Score: 1.0\n",
            "38.\tLoss: 0.46933531761169434\tF1-Score: 1.0\n",
            "39.\tLoss: 0.46896886825561523\tF1-Score: 1.0\n",
            "40.\tLoss: 0.46470847725868225\tF1-Score: 1.0\n",
            "41.\tLoss: 0.46695366501808167\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4693206548690796\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4640204608440399\tF1-Score: 1.0\n",
            "44.\tLoss: 0.46214398741722107\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4476984143257141\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4566546082496643\tF1-Score: 1.0\n",
            "47.\tLoss: 0.45896250009536743\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4278852343559265\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4447045624256134\tF1-Score: 1.0\n",
            "50.\tLoss: 0.41957277059555054\tF1-Score: 1.0\n",
            "51.\tLoss: 0.4167840778827667\tF1-Score: 1.0\n",
            "52.\tLoss: 0.44728562235832214\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4362977147102356\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4113110303878784\tF1-Score: 1.0\n",
            "55.\tLoss: 0.43176957964897156\tF1-Score: 1.0\n",
            "56.\tLoss: 0.40712353587150574\tF1-Score: 1.0\n",
            "57.\tLoss: 0.39285600185394287\tF1-Score: 1.0\n",
            "58.\tLoss: 0.5265271663665771\tF1-Score: 1.0\n",
            "59.\tLoss: 0.3736781179904938\tF1-Score: 1.0\n",
            "60.\tLoss: 0.3662104606628418\tF1-Score: 1.0\n",
            "61.\tLoss: 0.37264931201934814\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 1.1557189226150513\tF1-Score: 0.0\n",
            "2.\tLoss: 1.1061012744903564\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0959160327911377\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0935441255569458\tF1-Score: 0.0\n",
            "5.\tLoss: 1.0931360721588135\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0927983522415161\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0922973155975342\tF1-Score: 0.0\n",
            "8.\tLoss: 1.091740369796753\tF1-Score: 0.0\n",
            "9.\tLoss: 1.091235876083374\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0908429622650146\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0905792713165283\tF1-Score: 0.0\n",
            "12.\tLoss: 1.0904399156570435\tF1-Score: 0.0\n",
            "13.\tLoss: 1.0904111862182617\tF1-Score: 0.0\n",
            "14.\tLoss: 1.090658187866211\tF1-Score: 0.0\n",
            "15.\tLoss: 1.090651273727417\tF1-Score: 0.0\n",
            "16.\tLoss: 1.0907680988311768\tF1-Score: 0.0\n",
            "17.\tLoss: 1.0910921096801758\tF1-Score: 0.0\n",
            "18.\tLoss: 1.0915478467941284\tF1-Score: 0.0\n",
            "19.\tLoss: 1.0920542478561401\tF1-Score: 0.0\n",
            "20.\tLoss: 1.0925920009613037\tF1-Score: 0.0\n",
            "21.\tLoss: 1.0943083763122559\tF1-Score: 0.0\n",
            "22.\tLoss: 1.0941569805145264\tF1-Score: 0.0\n",
            "23.\tLoss: 1.0944794416427612\tF1-Score: 0.0\n",
            "24.\tLoss: 1.0944644212722778\tF1-Score: 0.0\n",
            "25.\tLoss: 1.0955698490142822\tF1-Score: 0.0\n",
            "26.\tLoss: 1.0966793298721313\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 1.1268491744995117\tF1-Score: 0.0\n",
            "2.\tLoss: 1.0697057247161865\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0632083415985107\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0609697103500366\tF1-Score: 0.0\n",
            "5.\tLoss: 1.0599595308303833\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0596683025360107\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0603586435317993\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0617074966430664\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0634552240371704\tF1-Score: 0.0\n",
            "10.\tLoss: 1.065455436706543\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0676238536834717\tF1-Score: 0.0\n",
            "12.\tLoss: 1.071238398551941\tF1-Score: 0.0\n",
            "13.\tLoss: 1.0734748840332031\tF1-Score: 0.0\n",
            "14.\tLoss: 1.0756858587265015\tF1-Score: 0.0\n",
            "15.\tLoss: 1.0768437385559082\tF1-Score: 0.0\n",
            "16.\tLoss: 1.0819377899169922\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.9993035197257996\tF1-Score: 0.0\n",
            "2.\tLoss: 1.0209808349609375\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0301581621170044\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0352526903152466\tF1-Score: 0.0\n",
            "5.\tLoss: 1.0377672910690308\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0389553308486938\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0395450592041016\tF1-Score: 0.0\n",
            "8.\tLoss: 1.039900541305542\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0402034521102905\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0405515432357788\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0410047769546509\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6101931929588318\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5656620264053345\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5535145401954651\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5472775101661682\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5441333651542664\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5407675504684448\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5388482809066772\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5373666882514954\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5358462929725647\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5342851281166077\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5327340960502625\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5312104821205139\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5297124981880188\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5282333493232727\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5267658829689026\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5253045558929443\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5238439440727234\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5223800539970398\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5209087133407593\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5170628428459167\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5155128240585327\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5144186615943909\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5131110548973083\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5140637755393982\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5126140117645264\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5102279186248779\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5083123445510864\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5066394805908203\tF1-Score: 1.0\n",
            "29.\tLoss: 0.5043289661407471\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5017663240432739\tF1-Score: 1.0\n",
            "31.\tLoss: 0.5021184086799622\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49982213973999023\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4983115494251251\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49588677287101746\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4935324788093567\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4833025336265564\tF1-Score: 1.0\n",
            "37.\tLoss: 0.48585042357444763\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4878646731376648\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4861828088760376\tF1-Score: 1.0\n",
            "40.\tLoss: 0.47557637095451355\tF1-Score: 1.0\n",
            "41.\tLoss: 0.47878938913345337\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4696257710456848\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4802900552749634\tF1-Score: 1.0\n",
            "44.\tLoss: 0.46232330799102783\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4681631922721863\tF1-Score: 1.0\n",
            "46.\tLoss: 0.47579026222229004\tF1-Score: 1.0\n",
            "47.\tLoss: 0.3494003415107727\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4486682713031769\tF1-Score: 1.0\n",
            "49.\tLoss: 0.45223525166511536\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.9950699806213379\tF1-Score: 0.0\n",
            "2.\tLoss: 1.0682618618011475\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0793074369430542\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0794786214828491\tF1-Score: 0.0\n",
            "5.\tLoss: 1.0781326293945312\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0771210193634033\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0764422416687012\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0760343074798584\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0758816003799438\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0759687423706055\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0762697458267212\tF1-Score: 0.0\n",
            "12.\tLoss: 1.0768322944641113\tF1-Score: 0.0\n",
            "13.\tLoss: 1.0774534940719604\tF1-Score: 0.0\n",
            "14.\tLoss: 1.078220248222351\tF1-Score: 0.0\n",
            "15.\tLoss: 1.0790587663650513\tF1-Score: 0.0\n",
            "16.\tLoss: 1.0800012350082397\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.638259768486023\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6008602380752563\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5868315696716309\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5763235688209534\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5668326020240784\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5582966208457947\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5506245493888855\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5437251925468445\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5375524759292603\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5320557951927185\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5271823406219482\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5228774547576904\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5190872550010681\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5157586932182312\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5128422379493713\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5102914571762085\tF1-Score: 1.0\n",
            "17.\tLoss: 0.508063554763794\tF1-Score: 1.0\n",
            "18.\tLoss: 0.506119966506958\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5044257044792175\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5029497146606445\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5016646981239319\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5005459189414978\tF1-Score: 1.0\n",
            "23.\tLoss: 0.49957239627838135\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4987252354621887\tF1-Score: 1.0\n",
            "25.\tLoss: 0.49789488315582275\tF1-Score: 1.0\n",
            "26.\tLoss: 0.49732646346092224\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49682870507240295\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49639150500297546\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49600735306739807\tF1-Score: 1.0\n",
            "30.\tLoss: 0.49566972255706787\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4953728914260864\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49511194229125977\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4948824644088745\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4946807622909546\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4945034682750702\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4943475127220154\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49421051144599915\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49409013986587524\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4939843416213989\tF1-Score: 1.0\n",
            "40.\tLoss: 0.49389156699180603\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4938102066516876\tF1-Score: 1.0\n",
            "42.\tLoss: 0.49373891949653625\tF1-Score: 1.0\n",
            "43.\tLoss: 0.49367669224739075\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49362221360206604\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4935746490955353\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4935334622859955\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49349772930145264\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4934667646884918\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4934401214122772\tF1-Score: 1.0\n",
            "50.\tLoss: 0.49341726303100586\tF1-Score: 1.0\n",
            "51.\tLoss: 0.4933977425098419\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4933813512325287\tF1-Score: 1.0\n",
            "53.\tLoss: 0.49336758255958557\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4933561682701111\tF1-Score: 1.0\n",
            "55.\tLoss: 0.49334678053855896\tF1-Score: 1.0\n",
            "56.\tLoss: 0.49333927035331726\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4933334290981293\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4933290183544159\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4933258891105652\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49332395195961\tF1-Score: 1.0\n",
            "61.\tLoss: 0.4933229684829712\tF1-Score: 1.0\n",
            "62.\tLoss: 0.4933227598667145\tF1-Score: 1.0\n",
            "63.\tLoss: 0.49332356452941895\tF1-Score: 1.0\n",
            "64.\tLoss: 0.49332481622695923\tF1-Score: 1.0\n",
            "65.\tLoss: 0.49332669377326965\tF1-Score: 1.0\n",
            "66.\tLoss: 0.49332910776138306\tF1-Score: 1.0\n",
            "67.\tLoss: 0.4933319389820099\tF1-Score: 1.0\n",
            "68.\tLoss: 0.49333515763282776\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4933386743068695\tF1-Score: 1.0\n",
            "70.\tLoss: 0.49334248900413513\tF1-Score: 1.0\n",
            "71.\tLoss: 0.4933463931083679\tF1-Score: 1.0\n",
            "72.\tLoss: 0.4933505058288574\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.5807408690452576\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5589325428009033\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5505329966545105\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5436917543411255\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5376797914505005\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5323426127433777\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5276052951812744\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5234106183052063\tF1-Score: 1.0\n",
            "9.\tLoss: 0.519706130027771\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5164430737495422\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5135754346847534\tF1-Score: 1.0\n",
            "12.\tLoss: 0.511060357093811\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5088582038879395\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5069324374198914\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5052504539489746\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5037827491760254\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5025025606155396\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5013865828514099\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5004141926765442\tF1-Score: 1.0\n",
            "20.\tLoss: 0.49956679344177246\tF1-Score: 1.0\n",
            "21.\tLoss: 0.4988286793231964\tF1-Score: 1.0\n",
            "22.\tLoss: 0.49818548560142517\tF1-Score: 1.0\n",
            "23.\tLoss: 0.4976252317428589\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4971367418766022\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4967111349105835\tF1-Score: 1.0\n",
            "26.\tLoss: 0.49634018540382385\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4960165321826935\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49573448300361633\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49548831582069397\tF1-Score: 1.0\n",
            "30.\tLoss: 0.49527350068092346\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49508604407310486\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4949224293231964\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4947796165943146\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49465495347976685\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49454617500305176\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4944510757923126\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49436822533607483\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49429580569267273\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4942325949668884\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4941776692867279\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49412962794303894\tF1-Score: 1.0\n",
            "42.\tLoss: 0.494087815284729\tF1-Score: 1.0\n",
            "43.\tLoss: 0.49405136704444885\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4940197169780731\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49399226903915405\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4939684271812439\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49394771456718445\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4939299523830414\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4939146041870117\tF1-Score: 1.0\n",
            "50.\tLoss: 0.49390143156051636\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49389010667800903\tF1-Score: 1.0\n",
            "52.\tLoss: 0.49388042092323303\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4938722252845764\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4938651919364929\tF1-Score: 1.0\n",
            "55.\tLoss: 0.4938594102859497\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4938545525074005\tF1-Score: 1.0\n",
            "57.\tLoss: 0.493850439786911\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4938473105430603\tF1-Score: 1.0\n",
            "59.\tLoss: 0.49384456872940063\tF1-Score: 1.0\n",
            "60.\tLoss: 0.4938424229621887\tF1-Score: 1.0\n",
            "61.\tLoss: 0.49384087324142456\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49383968114852905\tF1-Score: 1.0\n",
            "63.\tLoss: 0.4938388168811798\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4938383400440216\tF1-Score: 1.0\n",
            "65.\tLoss: 0.4938379228115082\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4938379228115082\tF1-Score: 1.0\n",
            "67.\tLoss: 0.4938379228115082\tF1-Score: 1.0\n",
            "68.\tLoss: 0.49383822083473206\tF1-Score: 1.0\n",
            "69.\tLoss: 0.49383851885795593\tF1-Score: 1.0\n",
            "70.\tLoss: 0.493838906288147\tF1-Score: 1.0\n",
            "71.\tLoss: 0.4938395023345947\tF1-Score: 1.0\n",
            "72.\tLoss: 0.49383988976478577\tF1-Score: 1.0\n",
            "73.\tLoss: 0.4938404858112335\tF1-Score: 1.0\n",
            "74.\tLoss: 0.4938410520553589\tF1-Score: 1.0\n",
            "75.\tLoss: 0.49384164810180664\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7810875773429871\tF1-Score: 0.0\n",
            "2.\tLoss: 0.8054102659225464\tF1-Score: 0.0\n",
            "3.\tLoss: 0.8293878436088562\tF1-Score: 0.0\n",
            "4.\tLoss: 0.8536514639854431\tF1-Score: 0.0\n",
            "5.\tLoss: 0.8780303001403809\tF1-Score: 0.0\n",
            "6.\tLoss: 0.9021334648132324\tF1-Score: 0.0\n",
            "7.\tLoss: 0.9254997968673706\tF1-Score: 0.0\n",
            "8.\tLoss: 0.9476598501205444\tF1-Score: 0.0\n",
            "9.\tLoss: 0.9682114720344543\tF1-Score: 0.0\n",
            "10.\tLoss: 0.9868751764297485\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0035133361816406\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.9283021092414856\tF1-Score: 0.0\n",
            "2.\tLoss: 1.0554107427597046\tF1-Score: 0.0\n",
            "3.\tLoss: 1.1066395044326782\tF1-Score: 0.0\n",
            "4.\tLoss: 1.1189769506454468\tF1-Score: 0.0\n",
            "5.\tLoss: 1.1201611757278442\tF1-Score: 0.0\n",
            "6.\tLoss: 1.1191504001617432\tF1-Score: 0.0\n",
            "7.\tLoss: 1.1178277730941772\tF1-Score: 0.0\n",
            "8.\tLoss: 1.11646568775177\tF1-Score: 0.0\n",
            "9.\tLoss: 1.1151005029678345\tF1-Score: 0.0\n",
            "10.\tLoss: 1.1137526035308838\tF1-Score: 0.0\n",
            "11.\tLoss: 1.1124391555786133\tF1-Score: 0.0\n",
            "12.\tLoss: 1.111170768737793\tF1-Score: 0.0\n",
            "13.\tLoss: 1.1099525690078735\tF1-Score: 0.0\n",
            "14.\tLoss: 1.1087863445281982\tF1-Score: 0.0\n",
            "15.\tLoss: 1.1076716184616089\tF1-Score: 0.0\n",
            "16.\tLoss: 1.1066069602966309\tF1-Score: 0.0\n",
            "17.\tLoss: 1.1055909395217896\tF1-Score: 0.0\n",
            "18.\tLoss: 1.104621171951294\tF1-Score: 0.0\n",
            "19.\tLoss: 1.104184627532959\tF1-Score: 0.0\n",
            "20.\tLoss: 1.1036103963851929\tF1-Score: 0.0\n",
            "21.\tLoss: 1.1023576259613037\tF1-Score: 0.0\n",
            "22.\tLoss: 1.1012845039367676\tF1-Score: 0.0\n",
            "23.\tLoss: 1.1005494594573975\tF1-Score: 0.0\n",
            "24.\tLoss: 1.1004022359848022\tF1-Score: 0.0\n",
            "25.\tLoss: 1.0994417667388916\tF1-Score: 0.0\n",
            "26.\tLoss: 1.098512053489685\tF1-Score: 0.0\n",
            "27.\tLoss: 1.0983659029006958\tF1-Score: 0.0\n",
            "28.\tLoss: 1.0976513624191284\tF1-Score: 0.0\n",
            "29.\tLoss: 1.0969959497451782\tF1-Score: 0.0\n",
            "30.\tLoss: 1.0964468717575073\tF1-Score: 0.0\n",
            "31.\tLoss: 1.0959627628326416\tF1-Score: 0.0\n",
            "32.\tLoss: 1.0959538221359253\tF1-Score: 0.0\n",
            "33.\tLoss: 1.0952264070510864\tF1-Score: 0.0\n",
            "34.\tLoss: 1.0952386856079102\tF1-Score: 0.0\n",
            "35.\tLoss: 1.094598650932312\tF1-Score: 0.0\n",
            "36.\tLoss: 1.0945104360580444\tF1-Score: 0.0\n",
            "37.\tLoss: 1.094497799873352\tF1-Score: 0.0\n",
            "38.\tLoss: 1.0935235023498535\tF1-Score: 0.0\n",
            "39.\tLoss: 1.0933215618133545\tF1-Score: 0.0\n",
            "40.\tLoss: 1.0933197736740112\tF1-Score: 0.0\n",
            "41.\tLoss: 1.0930918455123901\tF1-Score: 0.0\n",
            "42.\tLoss: 1.0927222967147827\tF1-Score: 0.0\n",
            "43.\tLoss: 1.091591477394104\tF1-Score: 0.0\n",
            "44.\tLoss: 1.0863313674926758\tF1-Score: 0.0\n",
            "45.\tLoss: 1.0892068147659302\tF1-Score: 0.0\n",
            "46.\tLoss: 1.0956324338912964\tF1-Score: 0.0\n",
            "47.\tLoss: 1.0940200090408325\tF1-Score: 0.0\n",
            "48.\tLoss: 1.096427083015442\tF1-Score: 0.0\n",
            "49.\tLoss: 1.097886085510254\tF1-Score: 0.0\n",
            "50.\tLoss: 1.0979359149932861\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6689756512641907\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6508122086524963\tF1-Score: 1.0\n",
            "3.\tLoss: 0.6387967467308044\tF1-Score: 1.0\n",
            "4.\tLoss: 0.6277523636817932\tF1-Score: 1.0\n",
            "5.\tLoss: 0.6174151301383972\tF1-Score: 1.0\n",
            "6.\tLoss: 0.6076148152351379\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5989819169044495\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5907631516456604\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5828005075454712\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5751407742500305\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5678403973579407\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5609508156776428\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5545114278793335\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5485455989837646\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5430617332458496\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5380542874336243\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5335075259208679\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5293979048728943\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5256971120834351\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5223740339279175\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5193967819213867\tF1-Score: 1.0\n",
            "22.\tLoss: 0.516733705997467\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5143545269966125\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5122305750846863\tF1-Score: 1.0\n",
            "25.\tLoss: 0.510335385799408\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5086446404457092\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5071362853050232\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5057901740074158\tF1-Score: 1.0\n",
            "29.\tLoss: 0.5045885443687439\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5035154223442078\tF1-Score: 1.0\n",
            "31.\tLoss: 0.5025564432144165\tF1-Score: 1.0\n",
            "32.\tLoss: 0.5016988515853882\tF1-Score: 1.0\n",
            "33.\tLoss: 0.500931441783905\tF1-Score: 1.0\n",
            "34.\tLoss: 0.5002443194389343\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4996283948421478\tF1-Score: 1.0\n",
            "36.\tLoss: 0.49907588958740234\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49857988953590393\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49813416600227356\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4977332651615143\tF1-Score: 1.0\n",
            "40.\tLoss: 0.49737250804901123\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4970473647117615\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4967540502548218\tF1-Score: 1.0\n",
            "43.\tLoss: 0.49648940563201904\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49625012278556824\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49603375792503357\tF1-Score: 1.0\n",
            "46.\tLoss: 0.49583783745765686\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4956604242324829\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49549946188926697\tF1-Score: 1.0\n",
            "49.\tLoss: 0.49535322189331055\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4952203035354614\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49509942531585693\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4949893355369568\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4948889911174774\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4947974979877472\tF1-Score: 1.0\n",
            "55.\tLoss: 0.49471381306648254\tF1-Score: 1.0\n",
            "56.\tLoss: 0.49463745951652527\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4945675730705261\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4945034682750702\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4944446384906769\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49439069628715515\tF1-Score: 1.0\n",
            "61.\tLoss: 0.4943411350250244\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49429550766944885\tF1-Score: 1.0\n",
            "63.\tLoss: 0.49425360560417175\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4942149221897125\tF1-Score: 1.0\n",
            "65.\tLoss: 0.49417924880981445\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4941462278366089\tF1-Score: 1.0\n",
            "67.\tLoss: 0.4941157400608063\tF1-Score: 1.0\n",
            "68.\tLoss: 0.4940875172615051\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4940614402294159\tF1-Score: 1.0\n",
            "70.\tLoss: 0.4940372109413147\tF1-Score: 1.0\n",
            "71.\tLoss: 0.49401453137397766\tF1-Score: 1.0\n",
            "72.\tLoss: 0.4939936399459839\tF1-Score: 1.0\n",
            "73.\tLoss: 0.4939741790294647\tF1-Score: 1.0\n",
            "74.\tLoss: 0.49395594000816345\tF1-Score: 1.0\n",
            "75.\tLoss: 0.4939389228820801\tF1-Score: 1.0\n",
            "76.\tLoss: 0.4939230978488922\tF1-Score: 1.0\n",
            "77.\tLoss: 0.49390825629234314\tF1-Score: 1.0\n",
            "78.\tLoss: 0.49389439821243286\tF1-Score: 1.0\n",
            "79.\tLoss: 0.49388131499290466\tF1-Score: 1.0\n",
            "80.\tLoss: 0.49386918544769287\tF1-Score: 1.0\n",
            "81.\tLoss: 0.49385765194892883\tF1-Score: 1.0\n",
            "82.\tLoss: 0.4938468337059021\tF1-Score: 1.0\n",
            "83.\tLoss: 0.49383658170700073\tF1-Score: 1.0\n",
            "84.\tLoss: 0.4938269853591919\tF1-Score: 1.0\n",
            "85.\tLoss: 0.4938180148601532\tF1-Score: 1.0\n",
            "86.\tLoss: 0.49380943179130554\tF1-Score: 1.0\n",
            "87.\tLoss: 0.49380120635032654\tF1-Score: 1.0\n",
            "88.\tLoss: 0.4937936067581177\tF1-Score: 1.0\n",
            "89.\tLoss: 0.49378618597984314\tF1-Score: 1.0\n",
            "90.\tLoss: 0.49377933144569397\tF1-Score: 1.0\n",
            "91.\tLoss: 0.4937726855278015\tF1-Score: 1.0\n",
            "92.\tLoss: 0.49376633763313293\tF1-Score: 1.0\n",
            "93.\tLoss: 0.49376028776168823\tF1-Score: 1.0\n",
            "94.\tLoss: 0.4937545359134674\tF1-Score: 1.0\n",
            "95.\tLoss: 0.4937489628791809\tF1-Score: 1.0\n",
            "96.\tLoss: 0.4937435984611511\tF1-Score: 1.0\n",
            "97.\tLoss: 0.4937385022640228\tF1-Score: 1.0\n",
            "98.\tLoss: 0.49373364448547363\tF1-Score: 1.0\n",
            "99.\tLoss: 0.49372875690460205\tF1-Score: 1.0\n",
            "100.\tLoss: 0.4937242567539215\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6413552165031433\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6220137476921082\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5911004543304443\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5996478796005249\tF1-Score: 1.0\n",
            "5.\tLoss: 0.6062796711921692\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5937919616699219\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5833955407142639\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5642520189285278\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5616239309310913\tF1-Score: 1.0\n",
            "10.\tLoss: 0.559734582901001\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5548226833343506\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5464950203895569\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5375149846076965\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5316533446311951\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5275078415870667\tF1-Score: 1.0\n",
            "16.\tLoss: 0.522962749004364\tF1-Score: 1.0\n",
            "17.\tLoss: 0.526034951210022\tF1-Score: 1.0\n",
            "18.\tLoss: 0.522523820400238\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5202934741973877\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5190271735191345\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5136515498161316\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5105594992637634\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5089815258979797\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4896138608455658\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5033270716667175\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5023350715637207\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5066536068916321\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4902774691581726\tF1-Score: 1.0\n",
            "29.\tLoss: 0.5053660273551941\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5017240643501282\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4937841296195984\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4975315034389496\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49800509214401245\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4975124001502991\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4965154528617859\tF1-Score: 1.0\n",
            "36.\tLoss: 0.49542608857154846\tF1-Score: 1.0\n",
            "37.\tLoss: 0.496263325214386\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4820174276828766\tF1-Score: 1.0\n",
            "39.\tLoss: 0.49114418029785156\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.603203535079956\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5490844249725342\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5316634178161621\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5250886678695679\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5223501324653625\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5211005806922913\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5241883993148804\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5227986574172974\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5150394439697266\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5173685550689697\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5184325575828552\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5183020830154419\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5180824995040894\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5179082155227661\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5177087187767029\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5174579620361328\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5171632170677185\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5168378353118896\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5164913535118103\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5161295533180237\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5157564878463745\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5153751373291016\tF1-Score: 1.0\n",
            "23.\tLoss: 0.514988362789154\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5145990252494812\tF1-Score: 1.0\n",
            "25.\tLoss: 0.514209508895874\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5137112140655518\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5042614340782166\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5048478841781616\tF1-Score: 1.0\n",
            "29.\tLoss: 0.5113530158996582\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4922695457935333\tF1-Score: 1.0\n",
            "31.\tLoss: 0.5019931793212891\tF1-Score: 1.0\n",
            "32.\tLoss: 0.48129647970199585\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49622827768325806\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49434515833854675\tF1-Score: 1.0\n",
            "35.\tLoss: 0.48522740602493286\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4686541259288788\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4810132384300232\tF1-Score: 1.0\n",
            "38.\tLoss: 0.48827388882637024\tF1-Score: 1.0\n",
            "39.\tLoss: 0.49090448021888733\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.5770494341850281\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5695927739143372\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5609109401702881\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5544156432151794\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5494416952133179\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5422344207763672\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5392012000083923\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5304893255233765\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5313767790794373\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5262435674667358\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5330838561058044\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5270067453384399\tF1-Score: 1.0\n",
            "13.\tLoss: 0.509377658367157\tF1-Score: 1.0\n",
            "14.\tLoss: 0.4886970520019531\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5368104577064514\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5305629372596741\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5253353714942932\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5210182666778564\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5172761082649231\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5140329003334045\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5112204551696777\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5087806582450867\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5066633224487305\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5048249363899231\tF1-Score: 1.0\n",
            "25.\tLoss: 0.503228485584259\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5018415451049805\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5006362199783325\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4995885193347931\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4986775517463684\tF1-Score: 1.0\n",
            "30.\tLoss: 0.49788525700569153\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49719610810279846\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4965965449810028\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4959500730037689\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49555209279060364\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4952050447463989\tF1-Score: 1.0\n",
            "36.\tLoss: 0.49490150809288025\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49463582038879395\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4944033920764923\tF1-Score: 1.0\n",
            "39.\tLoss: 0.49420034885406494\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4940227270126343\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49386781454086304\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4937325716018677\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4936147928237915\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49351227283477783\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49342313408851624\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4933459162712097\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49327903985977173\tF1-Score: 1.0\n",
            "48.\tLoss: 0.493221253156662\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4931715726852417\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4931289255619049\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49309250712394714\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4930615723133087\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4930354356765747\tF1-Score: 1.0\n",
            "54.\tLoss: 0.493013471364975\tF1-Score: 1.0\n",
            "55.\tLoss: 0.4929952323436737\tF1-Score: 1.0\n",
            "56.\tLoss: 0.49298039078712463\tF1-Score: 1.0\n",
            "57.\tLoss: 0.49296849966049194\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4929589331150055\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4929516911506653\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49294644594192505\tF1-Score: 1.0\n",
            "61.\tLoss: 0.49294281005859375\tF1-Score: 1.0\n",
            "62.\tLoss: 0.4929407835006714\tF1-Score: 1.0\n",
            "63.\tLoss: 0.4929399788379669\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4929405748844147\tF1-Score: 1.0\n",
            "65.\tLoss: 0.49294203519821167\tF1-Score: 1.0\n",
            "66.\tLoss: 0.49294447898864746\tF1-Score: 1.0\n",
            "67.\tLoss: 0.4929477870464325\tF1-Score: 1.0\n",
            "68.\tLoss: 0.49295151233673096\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4929561913013458\tF1-Score: 1.0\n",
            "70.\tLoss: 0.49296125769615173\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6924469470977783\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5700799226760864\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5452776551246643\tF1-Score: 1.0\n",
            "4.\tLoss: 0.554862380027771\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5436849594116211\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5369659662246704\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5333883166313171\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5309765338897705\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5287383794784546\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5273115038871765\tF1-Score: 1.0\n",
            "11.\tLoss: 0.525796115398407\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5241981148719788\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5203336477279663\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5201218724250793\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5174732804298401\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5175632834434509\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5151618719100952\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5127975344657898\tF1-Score: 1.0\n",
            "19.\tLoss: 0.51378333568573\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5141350626945496\tF1-Score: 1.0\n",
            "21.\tLoss: 0.510828971862793\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5111789107322693\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5086866617202759\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5092247128486633\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5068148970603943\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5071114897727966\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5073272585868835\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5043627023696899\tF1-Score: 1.0\n",
            "29.\tLoss: 0.5048179626464844\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5053057670593262\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 1.2833364009857178\tF1-Score: 0.0\n",
            "2.\tLoss: 1.1057145595550537\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0677869319915771\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0709285736083984\tF1-Score: 0.0\n",
            "5.\tLoss: 1.0760802030563354\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0792968273162842\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0818848609924316\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0845381021499634\tF1-Score: 0.0\n",
            "9.\tLoss: 1.087375283241272\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0903739929199219\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0935131311416626\tF1-Score: 0.0\n",
            "12.\tLoss: 1.096786379814148\tF1-Score: 0.0\n",
            "13.\tLoss: 1.100197196006775\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6081348657608032\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5853734016418457\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5721896290779114\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5616356134414673\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5527849197387695\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5452143549919128\tF1-Score: 1.0\n",
            "7.\tLoss: 0.538685142993927\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5330355167388916\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5281402468681335\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5238953232765198\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5202117562294006\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5170131921768188\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5142337083816528\tF1-Score: 1.0\n",
            "14.\tLoss: 0.511817216873169\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5097152590751648\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5078867077827454\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5062958598136902\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5049120187759399\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5037079453468323\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5026602149009705\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5017480850219727\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5009535551071167\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5002610087394714\tF1-Score: 1.0\n",
            "24.\tLoss: 0.49965718388557434\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4991300702095032\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4986695945262909\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4982669949531555\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49791496992111206\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4976066052913666\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4973364472389221\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49709948897361755\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4968913793563843\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49670839309692383\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4965475797653198\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49640560150146484\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4962804615497589\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4961697459220886\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4960717558860779\tF1-Score: 1.0\n",
            "39.\tLoss: 0.495984822511673\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4959075152873993\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4958385229110718\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4957769513130188\tF1-Score: 1.0\n",
            "43.\tLoss: 0.49572157859802246\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4956718683242798\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49562695622444153\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4955863356590271\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4955492615699768\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4955154061317444\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4954844117164612\tF1-Score: 1.0\n",
            "50.\tLoss: 0.49545565247535706\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49542924761772156\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4954043924808502\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4953812062740326\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4953593909740448\tF1-Score: 1.0\n",
            "55.\tLoss: 0.4953388571739197\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4953193962574005\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4953007102012634\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4952828884124756\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4952656924724579\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49524906277656555\tF1-Score: 1.0\n",
            "61.\tLoss: 0.49523282051086426\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49521708488464355\tF1-Score: 1.0\n",
            "63.\tLoss: 0.49520161747932434\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4951864778995514\tF1-Score: 1.0\n",
            "65.\tLoss: 0.49517160654067993\tF1-Score: 1.0\n",
            "66.\tLoss: 0.49515673518180847\tF1-Score: 1.0\n",
            "67.\tLoss: 0.495141863822937\tF1-Score: 1.0\n",
            "68.\tLoss: 0.49512720108032227\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4951125383377075\tF1-Score: 1.0\n",
            "70.\tLoss: 0.4950978755950928\tF1-Score: 1.0\n",
            "71.\tLoss: 0.4950830042362213\tF1-Score: 1.0\n",
            "72.\tLoss: 0.495068222284317\tF1-Score: 1.0\n",
            "73.\tLoss: 0.49505317211151123\tF1-Score: 1.0\n",
            "74.\tLoss: 0.49503791332244873\tF1-Score: 1.0\n",
            "75.\tLoss: 0.49502256512641907\tF1-Score: 1.0\n",
            "76.\tLoss: 0.4950067400932312\tF1-Score: 1.0\n",
            "77.\tLoss: 0.49499088525772095\tF1-Score: 1.0\n",
            "78.\tLoss: 0.49497464299201965\tF1-Score: 1.0\n",
            "79.\tLoss: 0.49495813250541687\tF1-Score: 1.0\n",
            "80.\tLoss: 0.4949414134025574\tF1-Score: 1.0\n",
            "81.\tLoss: 0.4949241876602173\tF1-Score: 1.0\n",
            "82.\tLoss: 0.49490660429000854\tF1-Score: 1.0\n",
            "83.\tLoss: 0.4948888123035431\tF1-Score: 1.0\n",
            "84.\tLoss: 0.49487051367759705\tF1-Score: 1.0\n",
            "85.\tLoss: 0.4948520362377167\tF1-Score: 1.0\n",
            "86.\tLoss: 0.49483299255371094\tF1-Score: 1.0\n",
            "87.\tLoss: 0.4948137104511261\tF1-Score: 1.0\n",
            "88.\tLoss: 0.49479398131370544\tF1-Score: 1.0\n",
            "89.\tLoss: 0.49477383494377136\tF1-Score: 1.0\n",
            "90.\tLoss: 0.4947533905506134\tF1-Score: 1.0\n",
            "91.\tLoss: 0.4947325885295868\tF1-Score: 1.0\n",
            "92.\tLoss: 0.4947114586830139\tF1-Score: 1.0\n",
            "93.\tLoss: 0.49469006061553955\tF1-Score: 1.0\n",
            "94.\tLoss: 0.49466827511787415\tF1-Score: 1.0\n",
            "95.\tLoss: 0.4946461617946625\tF1-Score: 1.0\n",
            "96.\tLoss: 0.49462369084358215\tF1-Score: 1.0\n",
            "97.\tLoss: 0.4946011006832123\tF1-Score: 1.0\n",
            "98.\tLoss: 0.4945782423019409\tF1-Score: 1.0\n",
            "99.\tLoss: 0.49455517530441284\tF1-Score: 1.0\n",
            "100.\tLoss: 0.4945318102836609\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.5396748185157776\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5358312129974365\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5442080497741699\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5352064371109009\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5244330763816833\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5187471508979797\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5142234563827515\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5104933381080627\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5074087381362915\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5048593282699585\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5027533173561096\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5010145902633667\tF1-Score: 1.0\n",
            "13.\tLoss: 0.4995792806148529\tF1-Score: 1.0\n",
            "14.\tLoss: 0.4983944296836853\tF1-Score: 1.0\n",
            "15.\tLoss: 0.4974164366722107\tF1-Score: 1.0\n",
            "16.\tLoss: 0.4966091811656952\tF1-Score: 1.0\n",
            "17.\tLoss: 0.4959428310394287\tF1-Score: 1.0\n",
            "18.\tLoss: 0.4953928291797638\tF1-Score: 1.0\n",
            "19.\tLoss: 0.4949389696121216\tF1-Score: 1.0\n",
            "20.\tLoss: 0.49456456303596497\tF1-Score: 1.0\n",
            "21.\tLoss: 0.494255930185318\tF1-Score: 1.0\n",
            "22.\tLoss: 0.4940018355846405\tF1-Score: 1.0\n",
            "23.\tLoss: 0.4937928020954132\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4936213195323944\tF1-Score: 1.0\n",
            "25.\tLoss: 0.49348101019859314\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4933665096759796\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49327367544174194\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49319881200790405\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4931388795375824\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4930916428565979\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49305495619773865\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4930270314216614\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49300655722618103\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4929922819137573\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49298331141471863\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4929788410663605\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49297815561294556\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4929807782173157\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4929860532283783\tF1-Score: 1.0\n",
            "40.\tLoss: 0.49299365282058716\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49300360679626465\tF1-Score: 1.0\n",
            "42.\tLoss: 0.49301502108573914\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4930281937122345\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49304264783859253\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4930582642555237\tF1-Score: 1.0\n",
            "46.\tLoss: 0.49307504296302795\tF1-Score: 1.0\n",
            "[2]\t0.6316\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7923469543457031\tF1-Score: 0.0\n",
            "2.\tLoss: 0.7186577916145325\tF1-Score: 0.0\n",
            "3.\tLoss: 0.5724894404411316\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5290055274963379\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5124011039733887\tF1-Score: 1.0\n",
            "6.\tLoss: 0.50639408826828\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5039423704147339\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5026341676712036\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5013983845710754\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5009967684745789\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5001598000526428\tF1-Score: 1.0\n",
            "12.\tLoss: 0.4989244043827057\tF1-Score: 1.0\n",
            "13.\tLoss: 0.49895179271698\tF1-Score: 1.0\n",
            "14.\tLoss: 0.49831998348236084\tF1-Score: 1.0\n",
            "15.\tLoss: 0.49706539511680603\tF1-Score: 1.0\n",
            "16.\tLoss: 0.4960215389728546\tF1-Score: 1.0\n",
            "17.\tLoss: 0.49325355887413025\tF1-Score: 1.0\n",
            "18.\tLoss: 0.49450016021728516\tF1-Score: 1.0\n",
            "19.\tLoss: 0.49353715777397156\tF1-Score: 1.0\n",
            "20.\tLoss: 0.4922468364238739\tF1-Score: 1.0\n",
            "21.\tLoss: 0.4906269907951355\tF1-Score: 1.0\n",
            "22.\tLoss: 0.48917356133461\tF1-Score: 1.0\n",
            "23.\tLoss: 0.48827075958251953\tF1-Score: 1.0\n",
            "24.\tLoss: 0.48639634251594543\tF1-Score: 1.0\n",
            "25.\tLoss: 0.48639053106307983\tF1-Score: 1.0\n",
            "26.\tLoss: 0.48558130860328674\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4843719005584717\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4818860590457916\tF1-Score: 1.0\n",
            "29.\tLoss: 0.48124825954437256\tF1-Score: 1.0\n",
            "30.\tLoss: 0.47922283411026\tF1-Score: 1.0\n",
            "31.\tLoss: 0.478728711605072\tF1-Score: 1.0\n",
            "32.\tLoss: 0.47660255432128906\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4759306013584137\tF1-Score: 1.0\n",
            "34.\tLoss: 0.47540318965911865\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4733467996120453\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4709600508213043\tF1-Score: 1.0\n",
            "37.\tLoss: 0.46892789006233215\tF1-Score: 1.0\n",
            "38.\tLoss: 0.46702858805656433\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4657609760761261\tF1-Score: 1.0\n",
            "40.\tLoss: 0.46361690759658813\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4612598717212677\tF1-Score: 1.0\n",
            "42.\tLoss: 0.458101749420166\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4560942053794861\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4531623423099518\tF1-Score: 1.0\n",
            "45.\tLoss: 0.45100119709968567\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4485505521297455\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4549587070941925\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4435904920101166\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4416576325893402\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4287305474281311\tF1-Score: 1.0\n",
            "51.\tLoss: 0.5765634179115295\tF1-Score: 1.0\n",
            "52.\tLoss: 0.388397753238678\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4067251682281494\tF1-Score: 1.0\n",
            "54.\tLoss: 0.3910331428050995\tF1-Score: 1.0\n",
            "55.\tLoss: 0.4236835241317749\tF1-Score: 1.0\n",
            "56.\tLoss: 0.3803477883338928\tF1-Score: 1.0\n",
            "57.\tLoss: 0.562211811542511\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4008737802505493\tF1-Score: 1.0\n",
            "59.\tLoss: 0.3611094057559967\tF1-Score: 1.0\n",
            "60.\tLoss: 0.5630773305892944\tF1-Score: 1.0\n",
            "61.\tLoss: 0.5239319205284119\tF1-Score: 1.0\n",
            "62.\tLoss: 0.3368469476699829\tF1-Score: 1.0\n",
            "63.\tLoss: 0.35088759660720825\tF1-Score: 1.0\n",
            "64.\tLoss: 0.3688372075557709\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.565144419670105\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5559232831001282\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5472409725189209\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5397143959999084\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5332579612731934\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5277172327041626\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5229566693305969\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5188627243041992\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5153394937515259\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5123066902160645\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5096943378448486\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5074433088302612\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5055018067359924\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5038254261016846\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5023762583732605\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5011215806007385\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5000333786010742\tF1-Score: 1.0\n",
            "18.\tLoss: 0.4990880489349365\tF1-Score: 1.0\n",
            "19.\tLoss: 0.49826502799987793\tF1-Score: 1.0\n",
            "20.\tLoss: 0.4975473880767822\tF1-Score: 1.0\n",
            "21.\tLoss: 0.4969203770160675\tF1-Score: 1.0\n",
            "22.\tLoss: 0.4963715076446533\tF1-Score: 1.0\n",
            "23.\tLoss: 0.4958902895450592\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4954673945903778\tF1-Score: 1.0\n",
            "25.\tLoss: 0.49509552121162415\tF1-Score: 1.0\n",
            "26.\tLoss: 0.49476778507232666\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49447864294052124\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49422311782836914\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4939969480037689\tF1-Score: 1.0\n",
            "30.\tLoss: 0.49379652738571167\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49361860752105713\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49346041679382324\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4933167099952698\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4931964576244354\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49308812618255615\tF1-Score: 1.0\n",
            "36.\tLoss: 0.49299055337905884\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4929025173187256\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4928230941295624\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4927510917186737\tF1-Score: 1.0\n",
            "40.\tLoss: 0.49268603324890137\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49262720346450806\tF1-Score: 1.0\n",
            "42.\tLoss: 0.49257394671440125\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4925256669521332\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49248185753822327\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49244225025177\tF1-Score: 1.0\n",
            "46.\tLoss: 0.49240636825561523\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49237361550331116\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49234405159950256\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4923171401023865\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4922926723957062\tF1-Score: 1.0\n",
            "51.\tLoss: 0.4922703206539154\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4922499358654022\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4922313094139099\tF1-Score: 1.0\n",
            "54.\tLoss: 0.49221426248550415\tF1-Score: 1.0\n",
            "55.\tLoss: 0.49219855666160583\tF1-Score: 1.0\n",
            "56.\tLoss: 0.492184042930603\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4921707808971405\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4921583831310272\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4921467900276184\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49213606119155884\tF1-Score: 1.0\n",
            "61.\tLoss: 0.492125928401947\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49211636185646057\tF1-Score: 1.0\n",
            "63.\tLoss: 0.4921073913574219\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4920988082885742\tF1-Score: 1.0\n",
            "65.\tLoss: 0.4920909106731415\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4920833110809326\tF1-Score: 1.0\n",
            "67.\tLoss: 0.49207618832588196\tF1-Score: 1.0\n",
            "68.\tLoss: 0.4920693635940552\tF1-Score: 1.0\n",
            "69.\tLoss: 0.49206283688545227\tF1-Score: 1.0\n",
            "70.\tLoss: 0.4920566976070404\tF1-Score: 1.0\n",
            "71.\tLoss: 0.4920508563518524\tF1-Score: 1.0\n",
            "72.\tLoss: 0.4920452833175659\tF1-Score: 1.0\n",
            "73.\tLoss: 0.49204012751579285\tF1-Score: 1.0\n",
            "74.\tLoss: 0.4920351505279541\tF1-Score: 1.0\n",
            "75.\tLoss: 0.4920305609703064\tF1-Score: 1.0\n",
            "76.\tLoss: 0.49202626943588257\tF1-Score: 1.0\n",
            "77.\tLoss: 0.4920223653316498\tF1-Score: 1.0\n",
            "78.\tLoss: 0.4920186698436737\tF1-Score: 1.0\n",
            "79.\tLoss: 0.4920152723789215\tF1-Score: 1.0\n",
            "80.\tLoss: 0.49201223254203796\tF1-Score: 1.0\n",
            "81.\tLoss: 0.49200931191444397\tF1-Score: 1.0\n",
            "82.\tLoss: 0.4920068681240082\tF1-Score: 1.0\n",
            "83.\tLoss: 0.49200472235679626\tF1-Score: 1.0\n",
            "84.\tLoss: 0.4920026957988739\tF1-Score: 1.0\n",
            "85.\tLoss: 0.49200111627578735\tF1-Score: 1.0\n",
            "86.\tLoss: 0.4919997751712799\tF1-Score: 1.0\n",
            "87.\tLoss: 0.4919987916946411\tF1-Score: 1.0\n",
            "88.\tLoss: 0.4919978976249695\tF1-Score: 1.0\n",
            "89.\tLoss: 0.4919973313808441\tF1-Score: 1.0\n",
            "90.\tLoss: 0.49199703335762024\tF1-Score: 1.0\n",
            "91.\tLoss: 0.4919968247413635\tF1-Score: 1.0\n",
            "92.\tLoss: 0.49199703335762024\tF1-Score: 1.0\n",
            "93.\tLoss: 0.4919973313808441\tF1-Score: 1.0\n",
            "94.\tLoss: 0.4919978082180023\tF1-Score: 1.0\n",
            "95.\tLoss: 0.49199849367141724\tF1-Score: 1.0\n",
            "96.\tLoss: 0.4919993579387665\tF1-Score: 1.0\n",
            "97.\tLoss: 0.49200043082237244\tF1-Score: 1.0\n",
            "98.\tLoss: 0.49200162291526794\tF1-Score: 1.0\n",
            "99.\tLoss: 0.4920028746128082\tF1-Score: 1.0\n",
            "100.\tLoss: 0.4920043349266052\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.8989920020103455\tF1-Score: 0.0\n",
            "2.\tLoss: 0.9623714685440063\tF1-Score: 0.0\n",
            "3.\tLoss: 0.987128496170044\tF1-Score: 0.0\n",
            "4.\tLoss: 0.9956129193305969\tF1-Score: 0.0\n",
            "5.\tLoss: 1.0097405910491943\tF1-Score: 0.0\n",
            "6.\tLoss: 1.013956069946289\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0217586755752563\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0279719829559326\tF1-Score: 0.0\n",
            "9.\tLoss: 1.028096079826355\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0275236368179321\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0292607545852661\tF1-Score: 0.0\n",
            "12.\tLoss: 1.039193868637085\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.8677865862846375\tF1-Score: 0.0\n",
            "2.\tLoss: 0.8940250873565674\tF1-Score: 0.0\n",
            "3.\tLoss: 0.920167088508606\tF1-Score: 0.0\n",
            "4.\tLoss: 0.9442647099494934\tF1-Score: 0.0\n",
            "5.\tLoss: 0.9660167098045349\tF1-Score: 0.0\n",
            "6.\tLoss: 0.9854254722595215\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0025646686553955\tF1-Score: 0.0\n",
            "8.\tLoss: 1.017545223236084\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0305123329162598\tF1-Score: 0.0\n",
            "10.\tLoss: 1.041638731956482\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0511140823364258\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.9193761348724365\tF1-Score: 0.0\n",
            "2.\tLoss: 1.1423022747039795\tF1-Score: 0.0\n",
            "3.\tLoss: 1.1723095178604126\tF1-Score: 0.0\n",
            "4.\tLoss: 1.174831509590149\tF1-Score: 0.0\n",
            "5.\tLoss: 1.1716314554214478\tF1-Score: 0.0\n",
            "6.\tLoss: 1.169386625289917\tF1-Score: 0.0\n",
            "7.\tLoss: 1.1640093326568604\tF1-Score: 0.0\n",
            "8.\tLoss: 1.1658680438995361\tF1-Score: 0.0\n",
            "9.\tLoss: 1.16620934009552\tF1-Score: 0.0\n",
            "10.\tLoss: 1.1658371686935425\tF1-Score: 0.0\n",
            "11.\tLoss: 1.1654120683670044\tF1-Score: 0.0\n",
            "12.\tLoss: 1.1651052236557007\tF1-Score: 0.0\n",
            "13.\tLoss: 1.1649165153503418\tF1-Score: 0.0\n",
            "14.\tLoss: 1.1648259162902832\tF1-Score: 0.0\n",
            "15.\tLoss: 1.1648192405700684\tF1-Score: 0.0\n",
            "16.\tLoss: 1.1648881435394287\tF1-Score: 0.0\n",
            "17.\tLoss: 1.1650265455245972\tF1-Score: 0.0\n",
            "18.\tLoss: 1.1652299165725708\tF1-Score: 0.0\n",
            "19.\tLoss: 1.1654949188232422\tF1-Score: 0.0\n",
            "20.\tLoss: 1.1658191680908203\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.4270838499069214\tF1-Score: 1.0\n",
            "2.\tLoss: 0.4862366020679474\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5092223286628723\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5223269462585449\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5262510180473328\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5265316963195801\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5263285040855408\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5266523957252502\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5251628756523132\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5232610106468201\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5263650417327881\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5230803489685059\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5026869773864746\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5098015069961548\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5175400972366333\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5637370944023132\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.680308997631073\tF1-Score: 1.0\n",
            "2.\tLoss: 0.7173005938529968\tF1-Score: 0.0\n",
            "3.\tLoss: 0.7520161867141724\tF1-Score: 0.0\n",
            "4.\tLoss: 0.7850791811943054\tF1-Score: 0.0\n",
            "5.\tLoss: 0.8171307444572449\tF1-Score: 0.0\n",
            "6.\tLoss: 0.8483835458755493\tF1-Score: 0.0\n",
            "7.\tLoss: 0.878616452217102\tF1-Score: 0.0\n",
            "8.\tLoss: 0.907712459564209\tF1-Score: 0.0\n",
            "9.\tLoss: 0.9350114464759827\tF1-Score: 0.0\n",
            "10.\tLoss: 0.9599403142929077\tF1-Score: 0.0\n",
            "11.\tLoss: 0.9821324348449707\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7402384281158447\tF1-Score: 0.0\n",
            "2.\tLoss: 0.7061464190483093\tF1-Score: 0.0\n",
            "3.\tLoss: 0.6620606780052185\tF1-Score: 1.0\n",
            "4.\tLoss: 0.6168215274810791\tF1-Score: 1.0\n",
            "5.\tLoss: 0.58363276720047\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5637571215629578\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5523360371589661\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5453681349754333\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5407314896583557\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5375398993492126\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5345938205718994\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5321735143661499\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5301191806793213\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5283072590827942\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5266655087471008\tF1-Score: 1.0\n",
            "16.\tLoss: 0.525151789188385\tF1-Score: 1.0\n",
            "17.\tLoss: 0.52373868227005\tF1-Score: 1.0\n",
            "18.\tLoss: 0.522406816482544\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5211406350135803\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5199275612831116\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5187563300132751\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5176182985305786\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5165053009986877\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5154109597206116\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5143300890922546\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5132580399513245\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5121912360191345\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5111262202262878\tF1-Score: 1.0\n",
            "29.\tLoss: 0.5100605487823486\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5089914202690125\tF1-Score: 1.0\n",
            "31.\tLoss: 0.5079163312911987\tF1-Score: 1.0\n",
            "32.\tLoss: 0.5068366527557373\tF1-Score: 1.0\n",
            "33.\tLoss: 0.5057432055473328\tF1-Score: 1.0\n",
            "34.\tLoss: 0.5046406984329224\tF1-Score: 1.0\n",
            "35.\tLoss: 0.5035200715065002\tF1-Score: 1.0\n",
            "36.\tLoss: 0.5023790001869202\tF1-Score: 1.0\n",
            "37.\tLoss: 0.501236617565155\tF1-Score: 1.0\n",
            "38.\tLoss: 0.5000610947608948\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4988613724708557\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4976370930671692\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4963853359222412\tF1-Score: 1.0\n",
            "42.\tLoss: 0.49510371685028076\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4937813878059387\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4924371838569641\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4910655915737152\tF1-Score: 1.0\n",
            "46.\tLoss: 0.48963749408721924\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4881654977798462\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4866475760936737\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4850895404815674\tF1-Score: 1.0\n",
            "50.\tLoss: 0.48346075415611267\tF1-Score: 1.0\n",
            "51.\tLoss: 0.48177412152290344\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4800354242324829\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4782157838344574\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4763326346874237\tF1-Score: 1.0\n",
            "55.\tLoss: 0.47436168789863586\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4723149538040161\tF1-Score: 1.0\n",
            "57.\tLoss: 0.47018304467201233\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4679526686668396\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4656221568584442\tF1-Score: 1.0\n",
            "60.\tLoss: 0.46318507194519043\tF1-Score: 1.0\n",
            "61.\tLoss: 0.4606347978115082\tF1-Score: 1.0\n",
            "62.\tLoss: 0.457964152097702\tF1-Score: 1.0\n",
            "63.\tLoss: 0.45516636967658997\tF1-Score: 1.0\n",
            "64.\tLoss: 0.45223432779312134\tF1-Score: 1.0\n",
            "65.\tLoss: 0.43636465072631836\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4388892948627472\tF1-Score: 1.0\n",
            "67.\tLoss: 0.4477192163467407\tF1-Score: 1.0\n",
            "68.\tLoss: 0.43307605385780334\tF1-Score: 1.0\n",
            "69.\tLoss: 0.43797627091407776\tF1-Score: 1.0\n",
            "70.\tLoss: 0.5516256093978882\tF1-Score: 1.0\n",
            "71.\tLoss: 0.5608854293823242\tF1-Score: 1.0\n",
            "72.\tLoss: 0.5515894889831543\tF1-Score: 1.0\n",
            "73.\tLoss: 0.5215009450912476\tF1-Score: 1.0\n",
            "74.\tLoss: 0.4714500606060028\tF1-Score: 1.0\n",
            "75.\tLoss: 0.376087486743927\tF1-Score: 1.0\n",
            "76.\tLoss: 0.37926843762397766\tF1-Score: 1.0\n",
            "77.\tLoss: 0.3911680579185486\tF1-Score: 1.0\n",
            "78.\tLoss: 0.3823997974395752\tF1-Score: 1.0\n",
            "79.\tLoss: 0.39461657404899597\tF1-Score: 1.0\n",
            "80.\tLoss: 0.3802163898944855\tF1-Score: 1.0\n",
            "81.\tLoss: 0.3701299726963043\tF1-Score: 1.0\n",
            "82.\tLoss: 0.3902188241481781\tF1-Score: 1.0\n",
            "83.\tLoss: 0.37253716588020325\tF1-Score: 1.0\n",
            "84.\tLoss: 0.3610721230506897\tF1-Score: 1.0\n",
            "85.\tLoss: 0.3566598892211914\tF1-Score: 1.0\n",
            "86.\tLoss: 0.3540998697280884\tF1-Score: 1.0\n",
            "87.\tLoss: 0.3515273928642273\tF1-Score: 1.0\n",
            "88.\tLoss: 0.3356204330921173\tF1-Score: 1.0\n",
            "89.\tLoss: 0.6253880858421326\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.5677382349967957\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5552058219909668\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5444692969322205\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5355585217475891\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5281702876091003\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5220270752906799\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5169088244438171\tF1-Score: 1.0\n",
            "8.\tLoss: 0.512640118598938\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5090801119804382\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5061126351356506\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5036391019821167\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5015767216682434\tF1-Score: 1.0\n",
            "13.\tLoss: 0.49985572695732117\tF1-Score: 1.0\n",
            "14.\tLoss: 0.498418390750885\tF1-Score: 1.0\n",
            "15.\tLoss: 0.49721699953079224\tF1-Score: 1.0\n",
            "16.\tLoss: 0.4962121248245239\tF1-Score: 1.0\n",
            "17.\tLoss: 0.4953717291355133\tF1-Score: 1.0\n",
            "18.\tLoss: 0.49466875195503235\tF1-Score: 1.0\n",
            "19.\tLoss: 0.4940812587738037\tF1-Score: 1.0\n",
            "20.\tLoss: 0.49359115958213806\tF1-Score: 1.0\n",
            "21.\tLoss: 0.4931831955909729\tF1-Score: 1.0\n",
            "22.\tLoss: 0.4928445518016815\tF1-Score: 1.0\n",
            "23.\tLoss: 0.4925650656223297\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4923354685306549\tF1-Score: 1.0\n",
            "25.\tLoss: 0.49214833974838257\tF1-Score: 1.0\n",
            "26.\tLoss: 0.49199724197387695\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49187663197517395\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49178189039230347\tF1-Score: 1.0\n",
            "29.\tLoss: 0.491708904504776\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4916544258594513\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49161553382873535\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4915897250175476\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4915750026702881\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4915696382522583\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49157196283340454\tF1-Score: 1.0\n",
            "36.\tLoss: 0.49150463938713074\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49155229330062866\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49159878492355347\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4916432201862335\tF1-Score: 1.0\n",
            "40.\tLoss: 0.49168580770492554\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49172642827033997\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4917658269405365\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4918036460876465\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4918401837348938\tF1-Score: 1.0\n",
            "45.\tLoss: 0.491875559091568\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7689473032951355\tF1-Score: 0.0\n",
            "2.\tLoss: 0.8197014331817627\tF1-Score: 0.0\n",
            "3.\tLoss: 0.8700745701789856\tF1-Score: 0.0\n",
            "4.\tLoss: 0.9160885214805603\tF1-Score: 0.0\n",
            "5.\tLoss: 0.9560478329658508\tF1-Score: 0.0\n",
            "6.\tLoss: 0.9894165992736816\tF1-Score: 0.0\n",
            "7.\tLoss: 1.016437292098999\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0378139019012451\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0544410943984985\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0672202110290527\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0769610404968262\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7673255801200867\tF1-Score: 0.0\n",
            "2.\tLoss: 1.0496076345443726\tF1-Score: 0.0\n",
            "3.\tLoss: 1.1559085845947266\tF1-Score: 0.0\n",
            "4.\tLoss: 1.1728335618972778\tF1-Score: 0.0\n",
            "5.\tLoss: 1.1682230234146118\tF1-Score: 0.0\n",
            "6.\tLoss: 1.16293203830719\tF1-Score: 0.0\n",
            "7.\tLoss: 1.1590763330459595\tF1-Score: 0.0\n",
            "8.\tLoss: 1.1560777425765991\tF1-Score: 0.0\n",
            "9.\tLoss: 1.1543501615524292\tF1-Score: 0.0\n",
            "10.\tLoss: 1.1526316404342651\tF1-Score: 0.0\n",
            "11.\tLoss: 1.150834321975708\tF1-Score: 0.0\n",
            "12.\tLoss: 1.1498150825500488\tF1-Score: 0.0\n",
            "13.\tLoss: 1.1470707654953003\tF1-Score: 0.0\n",
            "14.\tLoss: 1.1444422006607056\tF1-Score: 0.0\n",
            "15.\tLoss: 1.1429070234298706\tF1-Score: 0.0\n",
            "16.\tLoss: 1.1417648792266846\tF1-Score: 0.0\n",
            "17.\tLoss: 1.1406733989715576\tF1-Score: 0.0\n",
            "18.\tLoss: 1.138949990272522\tF1-Score: 0.0\n",
            "19.\tLoss: 1.1375139951705933\tF1-Score: 0.0\n",
            "20.\tLoss: 1.1363050937652588\tF1-Score: 0.0\n",
            "21.\tLoss: 1.1351933479309082\tF1-Score: 0.0\n",
            "22.\tLoss: 1.1341392993927002\tF1-Score: 0.0\n",
            "23.\tLoss: 1.1361501216888428\tF1-Score: 0.0\n",
            "24.\tLoss: 1.1339178085327148\tF1-Score: 0.0\n",
            "25.\tLoss: 1.131827473640442\tF1-Score: 0.0\n",
            "26.\tLoss: 1.1303348541259766\tF1-Score: 0.0\n",
            "27.\tLoss: 1.1292167901992798\tF1-Score: 0.0\n",
            "28.\tLoss: 1.128816843032837\tF1-Score: 0.0\n",
            "29.\tLoss: 1.1277481317520142\tF1-Score: 0.0\n",
            "30.\tLoss: 1.126732587814331\tF1-Score: 0.0\n",
            "31.\tLoss: 1.1259851455688477\tF1-Score: 0.0\n",
            "32.\tLoss: 1.1252577304840088\tF1-Score: 0.0\n",
            "33.\tLoss: 1.1244285106658936\tF1-Score: 0.0\n",
            "34.\tLoss: 1.1236096620559692\tF1-Score: 0.0\n",
            "35.\tLoss: 1.1226857900619507\tF1-Score: 0.0\n",
            "36.\tLoss: 1.1219545602798462\tF1-Score: 0.0\n",
            "37.\tLoss: 1.121185541152954\tF1-Score: 0.0\n",
            "38.\tLoss: 1.120400309562683\tF1-Score: 0.0\n",
            "39.\tLoss: 1.119606375694275\tF1-Score: 0.0\n",
            "40.\tLoss: 1.1200950145721436\tF1-Score: 0.0\n",
            "41.\tLoss: 1.1181421279907227\tF1-Score: 0.0\n",
            "42.\tLoss: 1.1168652772903442\tF1-Score: 0.0\n",
            "43.\tLoss: 1.1162693500518799\tF1-Score: 0.0\n",
            "44.\tLoss: 1.124096393585205\tF1-Score: 0.0\n",
            "45.\tLoss: 1.1181461811065674\tF1-Score: 0.0\n",
            "46.\tLoss: 1.1135694980621338\tF1-Score: 0.0\n",
            "47.\tLoss: 1.111368179321289\tF1-Score: 0.0\n",
            "48.\tLoss: 1.1102845668792725\tF1-Score: 0.0\n",
            "49.\tLoss: 1.1095459461212158\tF1-Score: 0.0\n",
            "50.\tLoss: 1.1094170808792114\tF1-Score: 0.0\n",
            "51.\tLoss: 1.1092462539672852\tF1-Score: 0.0\n",
            "52.\tLoss: 1.1083579063415527\tF1-Score: 0.0\n",
            "53.\tLoss: 1.1075248718261719\tF1-Score: 0.0\n",
            "54.\tLoss: 1.1200209856033325\tF1-Score: 0.0\n",
            "55.\tLoss: 1.1103194952011108\tF1-Score: 0.0\n",
            "56.\tLoss: 1.1036685705184937\tF1-Score: 0.0\n",
            "57.\tLoss: 1.1010422706604004\tF1-Score: 0.0\n",
            "58.\tLoss: 1.1000397205352783\tF1-Score: 0.0\n",
            "59.\tLoss: 1.0995122194290161\tF1-Score: 0.0\n",
            "60.\tLoss: 1.1125514507293701\tF1-Score: 0.0\n",
            "61.\tLoss: 1.0774929523468018\tF1-Score: 0.0\n",
            "62.\tLoss: 1.08750319480896\tF1-Score: 0.0\n",
            "63.\tLoss: 1.0933890342712402\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6543229818344116\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6191815137863159\tF1-Score: 1.0\n",
            "3.\tLoss: 0.6014785170555115\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5870475769042969\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5839497447013855\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5606147646903992\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5623072981834412\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5566054582595825\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5505304932594299\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5469557046890259\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5405360460281372\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5353084206581116\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5379177927970886\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5606394410133362\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5529047250747681\tF1-Score: 1.0\n",
            "16.\tLoss: 0.546265184879303\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5402692556381226\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5348755717277527\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5300446152687073\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5257328152656555\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5218949913978577\tF1-Score: 1.0\n",
            "22.\tLoss: 0.518486738204956\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5154654383659363\tF1-Score: 1.0\n",
            "24.\tLoss: 0.512791097164154\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5104268789291382\tF1-Score: 1.0\n",
            "26.\tLoss: 0.508338987827301\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5064967274665833\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5048721432685852\tF1-Score: 1.0\n",
            "29.\tLoss: 0.5034402012825012\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5021784901618958\tF1-Score: 1.0\n",
            "31.\tLoss: 0.5010672211647034\tF1-Score: 1.0\n",
            "32.\tLoss: 0.5000885128974915\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49922680854797363\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4984681308269501\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49779996275901794\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4972117841243744\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49669399857521057\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49623796343803406\tF1-Score: 1.0\n",
            "39.\tLoss: 0.495836466550827\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4954829514026642\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49517160654067993\tF1-Score: 1.0\n",
            "42.\tLoss: 0.49489742517471313\tF1-Score: 1.0\n",
            "43.\tLoss: 0.49465593695640564\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49444326758384705\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4942561388015747\tF1-Score: 1.0\n",
            "46.\tLoss: 0.49409133195877075\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4939463436603546\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4938187003135681\tF1-Score: 1.0\n",
            "49.\tLoss: 0.49370628595352173\tF1-Score: 1.0\n",
            "50.\tLoss: 0.49360767006874084\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49352094531059265\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4934447109699249\tF1-Score: 1.0\n",
            "53.\tLoss: 0.49337801337242126\tF1-Score: 1.0\n",
            "54.\tLoss: 0.49331945180892944\tF1-Score: 1.0\n",
            "55.\tLoss: 0.493268221616745\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4932233989238739\tF1-Score: 1.0\n",
            "57.\tLoss: 0.493184357881546\tF1-Score: 1.0\n",
            "58.\tLoss: 0.49315038323402405\tF1-Score: 1.0\n",
            "59.\tLoss: 0.49312081933021545\tF1-Score: 1.0\n",
            "60.\tLoss: 0.493095338344574\tF1-Score: 1.0\n",
            "61.\tLoss: 0.4930731952190399\tF1-Score: 1.0\n",
            "62.\tLoss: 0.4930543601512909\tF1-Score: 1.0\n",
            "63.\tLoss: 0.493038147687912\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4930244982242584\tF1-Score: 1.0\n",
            "65.\tLoss: 0.4930129945278168\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4930034279823303\tF1-Score: 1.0\n",
            "67.\tLoss: 0.49299561977386475\tF1-Score: 1.0\n",
            "68.\tLoss: 0.49298936128616333\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4929842948913574\tF1-Score: 1.0\n",
            "70.\tLoss: 0.49298059940338135\tF1-Score: 1.0\n",
            "71.\tLoss: 0.4929778575897217\tF1-Score: 1.0\n",
            "72.\tLoss: 0.49297618865966797\tF1-Score: 1.0\n",
            "73.\tLoss: 0.4929753243923187\tF1-Score: 1.0\n",
            "74.\tLoss: 0.492975115776062\tF1-Score: 1.0\n",
            "75.\tLoss: 0.49297571182250977\tF1-Score: 1.0\n",
            "76.\tLoss: 0.4929768741130829\tF1-Score: 1.0\n",
            "77.\tLoss: 0.49297845363616943\tF1-Score: 1.0\n",
            "78.\tLoss: 0.4929806888103485\tF1-Score: 1.0\n",
            "79.\tLoss: 0.49298331141471863\tF1-Score: 1.0\n",
            "80.\tLoss: 0.49298614263534546\tF1-Score: 1.0\n",
            "81.\tLoss: 0.49298936128616333\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6010470390319824\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5833615660667419\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5758847594261169\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5696247816085815\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5629636645317078\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5570348501205444\tF1-Score: 1.0\n",
            "7.\tLoss: 0.545028030872345\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5404378175735474\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5366401672363281\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5332765579223633\tF1-Score: 1.0\n",
            "11.\tLoss: 0.530239462852478\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5274662375450134\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5249181985855103\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5225694179534912\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5204002857208252\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5183953642845154\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5165416598320007\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5148279666900635\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5132439136505127\tF1-Score: 1.0\n",
            "20.\tLoss: 0.511780321598053\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5104277729988098\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5091784596443176\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5080251097679138\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5069603323936462\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5059775710105896\tF1-Score: 1.0\n",
            "26.\tLoss: 0.505070686340332\tF1-Score: 1.0\n",
            "27.\tLoss: 0.504233717918396\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5034615993499756\tF1-Score: 1.0\n",
            "29.\tLoss: 0.502748966217041\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5020913481712341\tF1-Score: 1.0\n",
            "31.\tLoss: 0.5014843940734863\tF1-Score: 1.0\n",
            "32.\tLoss: 0.5009241700172424\tF1-Score: 1.0\n",
            "33.\tLoss: 0.5004069209098816\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4999292492866516\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49948811531066895\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4990805983543396\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4987037479877472\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4983554780483246\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4980335533618927\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4977356195449829\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49745994806289673\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4972049295902252\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4969685971736908\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49674972891807556\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49654680490493774\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4963587820529938\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49618440866470337\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49602261185646057\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4958723783493042\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4957329332828522\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49560338258743286\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4954829514026642\tF1-Score: 1.0\n",
            "53.\tLoss: 0.49537092447280884\tF1-Score: 1.0\n",
            "54.\tLoss: 0.49526676535606384\tF1-Score: 1.0\n",
            "55.\tLoss: 0.49516984820365906\tF1-Score: 1.0\n",
            "56.\tLoss: 0.49507948756217957\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4949955642223358\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4949171543121338\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4948440194129944\tF1-Score: 1.0\n",
            "60.\tLoss: 0.4947759807109833\tF1-Score: 1.0\n",
            "61.\tLoss: 0.4947124421596527\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49465301632881165\tF1-Score: 1.0\n",
            "63.\tLoss: 0.49459749460220337\tF1-Score: 1.0\n",
            "64.\tLoss: 0.49454569816589355\tF1-Score: 1.0\n",
            "65.\tLoss: 0.4944972097873688\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4944518804550171\tF1-Score: 1.0\n",
            "67.\tLoss: 0.494409441947937\tF1-Score: 1.0\n",
            "68.\tLoss: 0.4943696856498718\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4943322539329529\tF1-Score: 1.0\n",
            "70.\tLoss: 0.4942973852157593\tF1-Score: 1.0\n",
            "71.\tLoss: 0.49426454305648804\tF1-Score: 1.0\n",
            "72.\tLoss: 0.49423375725746155\tF1-Score: 1.0\n",
            "73.\tLoss: 0.4942048490047455\tF1-Score: 1.0\n",
            "74.\tLoss: 0.4941776692867279\tF1-Score: 1.0\n",
            "75.\tLoss: 0.4941522777080536\tF1-Score: 1.0\n",
            "76.\tLoss: 0.4941282570362091\tF1-Score: 1.0\n",
            "77.\tLoss: 0.49410566687583923\tF1-Score: 1.0\n",
            "78.\tLoss: 0.49408429861068726\tF1-Score: 1.0\n",
            "79.\tLoss: 0.4940641522407532\tF1-Score: 1.0\n",
            "80.\tLoss: 0.49404531717300415\tF1-Score: 1.0\n",
            "81.\tLoss: 0.49402743577957153\tF1-Score: 1.0\n",
            "82.\tLoss: 0.4940105378627777\tF1-Score: 1.0\n",
            "83.\tLoss: 0.4939945936203003\tF1-Score: 1.0\n",
            "84.\tLoss: 0.4939795732498169\tF1-Score: 1.0\n",
            "85.\tLoss: 0.4939652979373932\tF1-Score: 1.0\n",
            "86.\tLoss: 0.4939517378807068\tF1-Score: 1.0\n",
            "87.\tLoss: 0.4939389228820801\tF1-Score: 1.0\n",
            "88.\tLoss: 0.4939268231391907\tF1-Score: 1.0\n",
            "89.\tLoss: 0.49391528964042664\tF1-Score: 1.0\n",
            "90.\tLoss: 0.4939042627811432\tF1-Score: 1.0\n",
            "91.\tLoss: 0.4938938021659851\tF1-Score: 1.0\n",
            "92.\tLoss: 0.4938839375972748\tF1-Score: 1.0\n",
            "93.\tLoss: 0.4938744604587555\tF1-Score: 1.0\n",
            "94.\tLoss: 0.4938654899597168\tF1-Score: 1.0\n",
            "95.\tLoss: 0.49385687708854675\tF1-Score: 1.0\n",
            "96.\tLoss: 0.49384868144989014\tF1-Score: 1.0\n",
            "97.\tLoss: 0.49384087324142456\tF1-Score: 1.0\n",
            "98.\tLoss: 0.49383345246315\tF1-Score: 1.0\n",
            "99.\tLoss: 0.4938262104988098\tF1-Score: 1.0\n",
            "100.\tLoss: 0.49381929636001587\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.4657336175441742\tF1-Score: 1.0\n",
            "2.\tLoss: 0.49224916100502014\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5086423754692078\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5171930193901062\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5210955739021301\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5246551632881165\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5255047678947449\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5253777503967285\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5246501564979553\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5235475897789001\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5222412347793579\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5208523273468018\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5130466222763062\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5074048638343811\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5059940814971924\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5023937821388245\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5050240159034729\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5010610222816467\tF1-Score: 1.0\n",
            "19.\tLoss: 0.4980999529361725\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5031455755233765\tF1-Score: 1.0\n",
            "21.\tLoss: 0.49906018376350403\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5031954646110535\tF1-Score: 1.0\n",
            "23.\tLoss: 0.4958803057670593\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5020447373390198\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.5840712785720825\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5766488313674927\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5673382878303528\tF1-Score: 1.0\n",
            "4.\tLoss: 0.558795690536499\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5510542988777161\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5440903306007385\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5378614664077759\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5323178768157959\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5274056196212769\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5230697393417358\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5192549228668213\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5159076452255249\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5129716396331787\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5104272961616516\tF1-Score: 1.0\n",
            "15.\tLoss: 0.508202075958252\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5062583684921265\tF1-Score: 1.0\n",
            "17.\tLoss: 0.504562497138977\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5030837655067444\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5017954707145691\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5006733536720276\tF1-Score: 1.0\n",
            "21.\tLoss: 0.49969616532325745\tF1-Score: 1.0\n",
            "22.\tLoss: 0.4988453686237335\tF1-Score: 1.0\n",
            "23.\tLoss: 0.49810466170310974\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4974597692489624\tF1-Score: 1.0\n",
            "25.\tLoss: 0.49689823389053345\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4964093267917633\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49598345160484314\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4956124722957611\tF1-Score: 1.0\n",
            "29.\tLoss: 0.495289146900177\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4950075149536133\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4947619140148163\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49454784393310547\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4943612813949585\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49419859051704407\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49405673146247864\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4939330816268921\tF1-Score: 1.0\n",
            "37.\tLoss: 0.493825227022171\tF1-Score: 1.0\n",
            "38.\tLoss: 0.493731290102005\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4936494529247284\tF1-Score: 1.0\n",
            "40.\tLoss: 0.493578165769577\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49351608753204346\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4934621751308441\tF1-Score: 1.0\n",
            "43.\tLoss: 0.49341532588005066\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49337470531463623\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4933393597602844\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4933089017868042\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4932827651500702\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49326011538505554\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4932407736778259\tF1-Score: 1.0\n",
            "50.\tLoss: 0.493224173784256\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49321022629737854\tF1-Score: 1.0\n",
            "52.\tLoss: 0.493198424577713\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4931884706020355\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4931802749633789\tF1-Score: 1.0\n",
            "55.\tLoss: 0.4931735396385193\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4931681454181671\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4931640625\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4931609332561493\tF1-Score: 1.0\n",
            "59.\tLoss: 0.49315860867500305\tF1-Score: 1.0\n",
            "60.\tLoss: 0.4931572377681732\tF1-Score: 1.0\n",
            "61.\tLoss: 0.49315643310546875\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49315643310546875\tF1-Score: 1.0\n",
            "63.\tLoss: 0.49315693974494934\tF1-Score: 1.0\n",
            "64.\tLoss: 0.49315792322158813\tF1-Score: 1.0\n",
            "65.\tLoss: 0.49315938353538513\tF1-Score: 1.0\n",
            "66.\tLoss: 0.493161141872406\tF1-Score: 1.0\n",
            "67.\tLoss: 0.4931633770465851\tF1-Score: 1.0\n",
            "68.\tLoss: 0.4931658208370209\tF1-Score: 1.0\n",
            "69.\tLoss: 0.49316856265068054\tF1-Score: 1.0\n",
            "70.\tLoss: 0.49317148327827454\tF1-Score: 1.0\n",
            "71.\tLoss: 0.49317479133605957\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6626095175743103\tF1-Score: 1.0\n",
            "2.\tLoss: 0.6179826855659485\tF1-Score: 1.0\n",
            "3.\tLoss: 0.6016381978988647\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5828850865364075\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5548301935195923\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5600900650024414\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5395510196685791\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5499265193939209\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5470162034034729\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5917562246322632\tF1-Score: 1.0\n",
            "11.\tLoss: 0.573859453201294\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5593938827514648\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5473914742469788\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5374795794487\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5293230414390564\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5226285457611084\tF1-Score: 1.0\n",
            "17.\tLoss: 0.517144501209259\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5126573443412781\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5089876651763916\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5059871077537537\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5035327672958374\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5015243291854858\tF1-Score: 1.0\n",
            "23.\tLoss: 0.49987971782684326\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4985318183898926\tF1-Score: 1.0\n",
            "25.\tLoss: 0.49742624163627625\tF1-Score: 1.0\n",
            "26.\tLoss: 0.496518611907959\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4957728385925293\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49515965580940247\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49465516209602356\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4942397177219391\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49389761686325073\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4936157763004303\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4933834969997406\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49319225549697876\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49303483963012695\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4929054379463196\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4927991032600403\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49271225929260254\tF1-Score: 1.0\n",
            "39.\tLoss: 0.49264106154441833\tF1-Score: 1.0\n",
            "40.\tLoss: 0.492583304643631\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49253666400909424\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4924992322921753\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4924693703651428\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4924459755420685\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4924280345439911\tF1-Score: 1.0\n",
            "46.\tLoss: 0.49241456389427185\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4924049973487854\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49239856004714966\tF1-Score: 1.0\n",
            "49.\tLoss: 0.4923948645591736\tF1-Score: 1.0\n",
            "50.\tLoss: 0.49239349365234375\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49239397048950195\tF1-Score: 1.0\n",
            "52.\tLoss: 0.4923960268497467\tF1-Score: 1.0\n",
            "53.\tLoss: 0.49239954352378845\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4924042224884033\tF1-Score: 1.0\n",
            "55.\tLoss: 0.492409884929657\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4924163222312927\tF1-Score: 1.0\n",
            "57.\tLoss: 0.49242374300956726\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.8710914254188538\tF1-Score: 0.0\n",
            "2.\tLoss: 1.064164161682129\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0952396392822266\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0954208374023438\tF1-Score: 0.0\n",
            "5.\tLoss: 1.096837043762207\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0993618965148926\tF1-Score: 0.0\n",
            "7.\tLoss: 1.1023962497711182\tF1-Score: 0.0\n",
            "8.\tLoss: 1.1053024530410767\tF1-Score: 0.0\n",
            "9.\tLoss: 1.1087727546691895\tF1-Score: 0.0\n",
            "10.\tLoss: 1.1124098300933838\tF1-Score: 0.0\n",
            "11.\tLoss: 1.116377353668213\tF1-Score: 0.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6957072615623474\tF1-Score: 0.0\n",
            "2.\tLoss: 0.6810175776481628\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5989636182785034\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5874641537666321\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5785128474235535\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5702368021011353\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5637697577476501\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5589051246643066\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5551362633705139\tF1-Score: 1.0\n",
            "10.\tLoss: 0.552054226398468\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5484191179275513\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5456592440605164\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5425922274589539\tF1-Score: 1.0\n",
            "14.\tLoss: 0.538304328918457\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5358255505561829\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5319024324417114\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5298107266426086\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5259578824043274\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5236107110977173\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5193970799446106\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5203178524971008\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5172979831695557\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5006749629974365\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5102447271347046\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5392752289772034\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5346418619155884\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5303061604499817\tF1-Score: 1.0\n",
            "28.\tLoss: 0.5263113379478455\tF1-Score: 1.0\n",
            "29.\tLoss: 0.5226859450340271\tF1-Score: 1.0\n",
            "30.\tLoss: 0.5194551944732666\tF1-Score: 1.0\n",
            "31.\tLoss: 0.5165818333625793\tF1-Score: 1.0\n",
            "32.\tLoss: 0.5140010118484497\tF1-Score: 1.0\n",
            "33.\tLoss: 0.511685848236084\tF1-Score: 1.0\n",
            "34.\tLoss: 0.5096113681793213\tF1-Score: 1.0\n",
            "35.\tLoss: 0.5077537298202515\tF1-Score: 1.0\n",
            "36.\tLoss: 0.5060914754867554\tF1-Score: 1.0\n",
            "37.\tLoss: 0.5046049356460571\tF1-Score: 1.0\n",
            "38.\tLoss: 0.5032762289047241\tF1-Score: 1.0\n",
            "39.\tLoss: 0.5020894408226013\tF1-Score: 1.0\n",
            "40.\tLoss: 0.5010303854942322\tF1-Score: 1.0\n",
            "41.\tLoss: 0.5000860691070557\tF1-Score: 1.0\n",
            "42.\tLoss: 0.49924466013908386\tF1-Score: 1.0\n",
            "43.\tLoss: 0.49849560856819153\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4978293776512146\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49723726511001587\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4967115521430969\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49624502658843994\tF1-Score: 1.0\n",
            "48.\tLoss: 0.495831161737442\tF1-Score: 1.0\n",
            "49.\tLoss: 0.49546435475349426\tF1-Score: 1.0\n",
            "50.\tLoss: 0.49513933062553406\tF1-Score: 1.0\n",
            "51.\tLoss: 0.49485135078430176\tF1-Score: 1.0\n",
            "52.\tLoss: 0.49459633231163025\tF1-Score: 1.0\n",
            "53.\tLoss: 0.49437054991722107\tF1-Score: 1.0\n",
            "54.\tLoss: 0.49417054653167725\tF1-Score: 1.0\n",
            "55.\tLoss: 0.4939936399459839\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4938369691371918\tF1-Score: 1.0\n",
            "57.\tLoss: 0.49369847774505615\tF1-Score: 1.0\n",
            "58.\tLoss: 0.49357593059539795\tF1-Score: 1.0\n",
            "59.\tLoss: 0.49346745014190674\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49337178468704224\tF1-Score: 1.0\n",
            "61.\tLoss: 0.493287056684494\tF1-Score: 1.0\n",
            "62.\tLoss: 0.4932122826576233\tF1-Score: 1.0\n",
            "63.\tLoss: 0.49314621090888977\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4930879473686218\tF1-Score: 1.0\n",
            "65.\tLoss: 0.4930365979671478\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4929913282394409\tF1-Score: 1.0\n",
            "67.\tLoss: 0.4929516911506653\tF1-Score: 1.0\n",
            "68.\tLoss: 0.4929165840148926\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4928860366344452\tF1-Score: 1.0\n",
            "70.\tLoss: 0.4928591847419739\tF1-Score: 1.0\n",
            "71.\tLoss: 0.49283567070961\tF1-Score: 1.0\n",
            "72.\tLoss: 0.4928152859210968\tF1-Score: 1.0\n",
            "73.\tLoss: 0.49279752373695374\tF1-Score: 1.0\n",
            "74.\tLoss: 0.4927821159362793\tF1-Score: 1.0\n",
            "75.\tLoss: 0.4927688539028168\tF1-Score: 1.0\n",
            "76.\tLoss: 0.4927574396133423\tF1-Score: 1.0\n",
            "77.\tLoss: 0.49274778366088867\tF1-Score: 1.0\n",
            "78.\tLoss: 0.4927394986152649\tF1-Score: 1.0\n",
            "79.\tLoss: 0.4927326440811157\tF1-Score: 1.0\n",
            "80.\tLoss: 0.4927268922328949\tF1-Score: 1.0\n",
            "81.\tLoss: 0.49272242188453674\tF1-Score: 1.0\n",
            "82.\tLoss: 0.4927186071872711\tF1-Score: 1.0\n",
            "83.\tLoss: 0.4927157759666443\tF1-Score: 1.0\n",
            "84.\tLoss: 0.49271371960639954\tF1-Score: 1.0\n",
            "85.\tLoss: 0.49271225929260254\tF1-Score: 1.0\n",
            "86.\tLoss: 0.49271148443222046\tF1-Score: 1.0\n",
            "87.\tLoss: 0.4927111864089966\tF1-Score: 1.0\n",
            "88.\tLoss: 0.4927113950252533\tF1-Score: 1.0\n",
            "89.\tLoss: 0.49271196126937866\tF1-Score: 1.0\n",
            "90.\tLoss: 0.4927130341529846\tF1-Score: 1.0\n",
            "91.\tLoss: 0.4927143156528473\tF1-Score: 1.0\n",
            "92.\tLoss: 0.492715984582901\tF1-Score: 1.0\n",
            "93.\tLoss: 0.4927179217338562\tF1-Score: 1.0\n",
            "94.\tLoss: 0.4927200675010681\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6089116334915161\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5910782814025879\tF1-Score: 1.0\n",
            "3.\tLoss: 0.578233540058136\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5672733783721924\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5578144192695618\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5495980381965637\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5424400568008423\tF1-Score: 1.0\n",
            "8.\tLoss: 0.536198079586029\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5307551622390747\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5260113477706909\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5218795537948608\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5182822346687317\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5151510834693909\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5124257802963257\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5100533962249756\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5079872608184814\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5061869025230408\tF1-Score: 1.0\n",
            "18.\tLoss: 0.504616916179657\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5032468438148499\tF1-Score: 1.0\n",
            "20.\tLoss: 0.50204998254776\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5010034441947937\tF1-Score: 1.0\n",
            "22.\tLoss: 0.500087559223175\tF1-Score: 1.0\n",
            "23.\tLoss: 0.4992850124835968\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4985813796520233\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4979633390903473\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4974205493927002\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4969431161880493\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4965231120586395\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49615320563316345\tF1-Score: 1.0\n",
            "30.\tLoss: 0.49582746624946594\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4955403804779053\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49528759717941284\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49506479501724243\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4948383569717407\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4946790039539337\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4945378601551056\tF1-Score: 1.0\n",
            "37.\tLoss: 0.49441277980804443\tF1-Score: 1.0\n",
            "38.\tLoss: 0.49430185556411743\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4942035675048828\tF1-Score: 1.0\n",
            "40.\tLoss: 0.49411633610725403\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4940391480922699\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4939707815647125\tF1-Score: 1.0\n",
            "43.\tLoss: 0.4939101040363312\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4938564896583557\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4938088357448578\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4937664568424225\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4937289357185364\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49369555711746216\tF1-Score: 1.0\n",
            "49.\tLoss: 0.493665874004364\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4936392903327942\tF1-Score: 1.0\n",
            "51.\tLoss: 0.4936157763004303\tF1-Score: 1.0\n",
            "52.\tLoss: 0.49359458684921265\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4935758411884308\tF1-Score: 1.0\n",
            "54.\tLoss: 0.49355873465538025\tF1-Score: 1.0\n",
            "55.\tLoss: 0.493543416261673\tF1-Score: 1.0\n",
            "56.\tLoss: 0.493529736995697\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4935172498226166\tF1-Score: 1.0\n",
            "58.\tLoss: 0.49350592494010925\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4934956729412079\tF1-Score: 1.0\n",
            "60.\tLoss: 0.49348628520965576\tF1-Score: 1.0\n",
            "61.\tLoss: 0.4934779107570648\tF1-Score: 1.0\n",
            "62.\tLoss: 0.4934701919555664\tF1-Score: 1.0\n",
            "63.\tLoss: 0.4934632480144501\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4934569001197815\tF1-Score: 1.0\n",
            "65.\tLoss: 0.4934510588645935\tF1-Score: 1.0\n",
            "66.\tLoss: 0.4934457838535309\tF1-Score: 1.0\n",
            "67.\tLoss: 0.493441104888916\tF1-Score: 1.0\n",
            "68.\tLoss: 0.4934366047382355\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4934327006340027\tF1-Score: 1.0\n",
            "70.\tLoss: 0.4934292733669281\tF1-Score: 1.0\n",
            "71.\tLoss: 0.49342605471611023\tF1-Score: 1.0\n",
            "72.\tLoss: 0.49342331290245056\tF1-Score: 1.0\n",
            "73.\tLoss: 0.49342089891433716\tF1-Score: 1.0\n",
            "74.\tLoss: 0.4934188425540924\tF1-Score: 1.0\n",
            "75.\tLoss: 0.493416965007782\tF1-Score: 1.0\n",
            "76.\tLoss: 0.493415504693985\tF1-Score: 1.0\n",
            "77.\tLoss: 0.49341434240341187\tF1-Score: 1.0\n",
            "78.\tLoss: 0.4934134781360626\tF1-Score: 1.0\n",
            "79.\tLoss: 0.4934127926826477\tF1-Score: 1.0\n",
            "80.\tLoss: 0.49341240525245667\tF1-Score: 1.0\n",
            "81.\tLoss: 0.49341219663619995\tF1-Score: 1.0\n",
            "82.\tLoss: 0.49341219663619995\tF1-Score: 1.0\n",
            "83.\tLoss: 0.49341249465942383\tF1-Score: 1.0\n",
            "84.\tLoss: 0.4934130609035492\tF1-Score: 1.0\n",
            "85.\tLoss: 0.4934137463569641\tF1-Score: 1.0\n",
            "86.\tLoss: 0.49341464042663574\tF1-Score: 1.0\n",
            "87.\tLoss: 0.4934157133102417\tF1-Score: 1.0\n",
            "88.\tLoss: 0.493416965007782\tF1-Score: 1.0\n",
            "89.\tLoss: 0.49341845512390137\tF1-Score: 1.0\n",
            "90.\tLoss: 0.4934200048446655\tF1-Score: 1.0\n",
            "91.\tLoss: 0.4934215545654297\tF1-Score: 1.0\n",
            "[3]\t0.6316\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.5767555832862854\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5685617923736572\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5565049052238464\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5464301705360413\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5380105972290039\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5309253334999084\tF1-Score: 1.0\n",
            "7.\tLoss: 0.524932324886322\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5198465585708618\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5155197381973267\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5118308067321777\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5086797475814819\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5059827566146851\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5036706924438477\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5016852617263794\tF1-Score: 1.0\n",
            "15.\tLoss: 0.49997857213020325\tF1-Score: 1.0\n",
            "16.\tLoss: 0.49851012229919434\tF1-Score: 1.0\n",
            "17.\tLoss: 0.4972456991672516\tF1-Score: 1.0\n",
            "18.\tLoss: 0.4961562156677246\tF1-Score: 1.0\n",
            "19.\tLoss: 0.4952171742916107\tF1-Score: 1.0\n",
            "20.\tLoss: 0.4944070279598236\tF1-Score: 1.0\n",
            "21.\tLoss: 0.49370765686035156\tF1-Score: 1.0\n",
            "22.\tLoss: 0.4931032657623291\tF1-Score: 1.0\n",
            "23.\tLoss: 0.49258068203926086\tF1-Score: 1.0\n",
            "24.\tLoss: 0.49212825298309326\tF1-Score: 1.0\n",
            "25.\tLoss: 0.49173638224601746\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4913966953754425\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4911021888256073\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4908469319343567\tF1-Score: 1.0\n",
            "29.\tLoss: 0.4906255304813385\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4904337525367737\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49026793241500854\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4901246726512909\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4900011122226715\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4898952543735504\tF1-Score: 1.0\n",
            "35.\tLoss: 0.48980459570884705\tF1-Score: 1.0\n",
            "36.\tLoss: 0.48972755670547485\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4896625876426697\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4896083176136017\tF1-Score: 1.0\n",
            "39.\tLoss: 0.48956358432769775\tF1-Score: 1.0\n",
            "40.\tLoss: 0.48952728509902954\tF1-Score: 1.0\n",
            "41.\tLoss: 0.48949840664863586\tF1-Score: 1.0\n",
            "42.\tLoss: 0.48947635293006897\tF1-Score: 1.0\n",
            "43.\tLoss: 0.48946020007133484\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4894494116306305\tF1-Score: 1.0\n",
            "45.\tLoss: 0.48944327235221863\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4894414246082306\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4894433915615082\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4894486367702484\tF1-Score: 1.0\n",
            "49.\tLoss: 0.48945698142051697\tF1-Score: 1.0\n",
            "50.\tLoss: 0.489467978477478\tF1-Score: 1.0\n",
            "51.\tLoss: 0.4894813895225525\tF1-Score: 1.0\n",
            "52.\tLoss: 0.48949694633483887\tF1-Score: 1.0\n",
            "53.\tLoss: 0.48951455950737\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4895337224006653\tF1-Score: 1.0\n",
            "55.\tLoss: 0.48955443501472473\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4895766079425812\tF1-Score: 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7402632832527161\tF1-Score: 0.0\n",
            "2.\tLoss: 0.7009713649749756\tF1-Score: 0.0\n",
            "3.\tLoss: 0.6592661738395691\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5879050493240356\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5916413068771362\tF1-Score: 1.0\n",
            "6.\tLoss: 0.6174758076667786\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5965861678123474\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5786774158477783\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5634016990661621\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5504641532897949\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5395963191986084\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5305356979370117\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5230275988578796\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5168338418006897\tF1-Score: 1.0\n",
            "15.\tLoss: 0.5117397308349609\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5075575709342957\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5041268467903137\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5013130903244019\tF1-Score: 1.0\n",
            "19.\tLoss: 0.4990043044090271\tF1-Score: 1.0\n",
            "20.\tLoss: 0.497108519077301\tF1-Score: 1.0\n",
            "21.\tLoss: 0.4955796003341675\tF1-Score: 1.0\n",
            "22.\tLoss: 0.49439185857772827\tF1-Score: 1.0\n",
            "23.\tLoss: 0.49340730905532837\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4925883710384369\tF1-Score: 1.0\n",
            "25.\tLoss: 0.49190637469291687\tF1-Score: 1.0\n",
            "26.\tLoss: 0.4913378357887268\tF1-Score: 1.0\n",
            "27.\tLoss: 0.4908636808395386\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49046802520751953\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49013790488243103\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4898627698421478\tF1-Score: 1.0\n",
            "31.\tLoss: 0.48963358998298645\tF1-Score: 1.0\n",
            "32.\tLoss: 0.48944318294525146\tF1-Score: 1.0\n",
            "33.\tLoss: 0.4892854690551758\tF1-Score: 1.0\n",
            "34.\tLoss: 0.48915499448776245\tF1-Score: 1.0\n",
            "35.\tLoss: 0.4890478849411011\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4889602065086365\tF1-Score: 1.0\n",
            "37.\tLoss: 0.48888906836509705\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4888317286968231\tF1-Score: 1.0\n",
            "39.\tLoss: 0.48878607153892517\tF1-Score: 1.0\n",
            "40.\tLoss: 0.4887501895427704\tF1-Score: 1.0\n",
            "41.\tLoss: 0.4887225925922394\tF1-Score: 1.0\n",
            "42.\tLoss: 0.48870208859443665\tF1-Score: 1.0\n",
            "43.\tLoss: 0.488687127828598\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4886772334575653\tF1-Score: 1.0\n",
            "45.\tLoss: 0.48867160081863403\tF1-Score: 1.0\n",
            "46.\tLoss: 0.48866915702819824\tF1-Score: 1.0\n",
            "47.\tLoss: 0.48866966366767883\tF1-Score: 1.0\n",
            "48.\tLoss: 0.48867267370224\tF1-Score: 1.0\n",
            "49.\tLoss: 0.48867762088775635\tF1-Score: 1.0\n",
            "50.\tLoss: 0.48868411779403687\tF1-Score: 1.0\n",
            "51.\tLoss: 0.48869219422340393\tF1-Score: 1.0\n",
            "52.\tLoss: 0.48870131373405457\tF1-Score: 1.0\n",
            "53.\tLoss: 0.48871153593063354\tF1-Score: 1.0\n",
            "54.\tLoss: 0.48872241377830505\tF1-Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 1.053387999534607\tF1-Score: 0.0\n",
            "2.\tLoss: 1.1102609634399414\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0956162214279175\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0920945405960083\tF1-Score: 0.0\n",
            "5.\tLoss: 1.094652533531189\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0907871723175049\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0899657011032104\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0902565717697144\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0907196998596191\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0912854671478271\tF1-Score: 0.0\n",
            "11.\tLoss: 1.1064568758010864\tF1-Score: 0.0\n",
            "12.\tLoss: 1.0955479145050049\tF1-Score: 0.0\n",
            "13.\tLoss: 1.0970698595046997\tF1-Score: 0.0\n",
            "14.\tLoss: 1.0985872745513916\tF1-Score: 0.0\n",
            "15.\tLoss: 1.0990004539489746\tF1-Score: 0.0\n",
            "16.\tLoss: 1.0991848707199097\tF1-Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.9498424530029297\tF1-Score: 0.0\n",
            "2.\tLoss: 1.015076756477356\tF1-Score: 0.0\n",
            "3.\tLoss: 1.0304017066955566\tF1-Score: 0.0\n",
            "4.\tLoss: 1.0343464612960815\tF1-Score: 0.0\n",
            "5.\tLoss: 1.0367164611816406\tF1-Score: 0.0\n",
            "6.\tLoss: 1.0390146970748901\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0413140058517456\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0436182022094727\tF1-Score: 0.0\n",
            "9.\tLoss: 1.045960783958435\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0483750104904175\tF1-Score: 0.0\n",
            "11.\tLoss: 1.0508846044540405\tF1-Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.8396737575531006\tF1-Score: 0.0\n",
            "2.\tLoss: 1.1887954473495483\tF1-Score: 0.0\n",
            "3.\tLoss: 1.1957981586456299\tF1-Score: 0.0\n",
            "4.\tLoss: 1.1862523555755615\tF1-Score: 0.0\n",
            "5.\tLoss: 1.1949825286865234\tF1-Score: 0.0\n",
            "6.\tLoss: 1.7412728071212769\tF1-Score: 0.0\n",
            "7.\tLoss: 1.0904091596603394\tF1-Score: 0.0\n",
            "8.\tLoss: 1.1601852178573608\tF1-Score: 0.0\n",
            "9.\tLoss: 1.1831839084625244\tF1-Score: 0.0\n",
            "10.\tLoss: 1.1815979480743408\tF1-Score: 0.0\n",
            "11.\tLoss: 1.1800386905670166\tF1-Score: 0.0\n",
            "12.\tLoss: 1.185484766960144\tF1-Score: 0.0\n",
            "13.\tLoss: 1.1737347841262817\tF1-Score: 0.0\n",
            "14.\tLoss: 1.1786404848098755\tF1-Score: 0.0\n",
            "15.\tLoss: 1.1796715259552002\tF1-Score: 0.0\n",
            "16.\tLoss: 1.1792218685150146\tF1-Score: 0.0\n",
            "17.\tLoss: 1.1790810823440552\tF1-Score: 0.0\n",
            "18.\tLoss: 1.1793512105941772\tF1-Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.6071006059646606\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5898905992507935\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5787035226821899\tF1-Score: 1.0\n",
            "4.\tLoss: 0.568732500076294\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5598263740539551\tF1-Score: 1.0\n",
            "6.\tLoss: 0.551868736743927\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5447714924812317\tF1-Score: 1.0\n",
            "8.\tLoss: 0.5384629964828491\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5328773856163025\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5279505848884583\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5236191749572754\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5198211073875427\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5164977312088013\tF1-Score: 1.0\n",
            "14.\tLoss: 0.513593852519989\tF1-Score: 1.0\n",
            "15.\tLoss: 0.511059582233429\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5088493824005127\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5069231390953064\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5052447319030762\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5037826299667358\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5025088787078857\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5013985633850098\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5004305243492126\tF1-Score: 1.0\n",
            "23.\tLoss: 0.49958595633506775\tF1-Score: 1.0\n",
            "24.\tLoss: 0.4988483190536499\tF1-Score: 1.0\n",
            "25.\tLoss: 0.4982036352157593\tF1-Score: 1.0\n",
            "26.\tLoss: 0.49763965606689453\tF1-Score: 1.0\n",
            "27.\tLoss: 0.49714574217796326\tF1-Score: 1.0\n",
            "28.\tLoss: 0.49671292304992676\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49633315205574036\tF1-Score: 1.0\n",
            "30.\tLoss: 0.49599969387054443\tF1-Score: 1.0\n",
            "31.\tLoss: 0.49570631980895996\tF1-Score: 1.0\n",
            "32.\tLoss: 0.49544820189476013\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49522069096565247\tF1-Score: 1.0\n",
            "34.\tLoss: 0.49502021074295044\tF1-Score: 1.0\n",
            "35.\tLoss: 0.49484315514564514\tF1-Score: 1.0\n",
            "36.\tLoss: 0.49468672275543213\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4945484399795532\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4944261610507965\tF1-Score: 1.0\n",
            "39.\tLoss: 0.49431779980659485\tF1-Score: 1.0\n",
            "40.\tLoss: 0.49422183632850647\tF1-Score: 1.0\n",
            "41.\tLoss: 0.49413684010505676\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4940614402294159\tF1-Score: 1.0\n",
            "43.\tLoss: 0.49399471282958984\tF1-Score: 1.0\n",
            "44.\tLoss: 0.49393561482429504\tF1-Score: 1.0\n",
            "45.\tLoss: 0.49388325214385986\tF1-Score: 1.0\n",
            "46.\tLoss: 0.4938369691371918\tF1-Score: 1.0\n",
            "47.\tLoss: 0.49379613995552063\tF1-Score: 1.0\n",
            "48.\tLoss: 0.49376019835472107\tF1-Score: 1.0\n",
            "49.\tLoss: 0.49372854828834534\tF1-Score: 1.0\n",
            "50.\tLoss: 0.4937008321285248\tF1-Score: 1.0\n",
            "51.\tLoss: 0.4936768114566803\tF1-Score: 1.0\n",
            "52.\tLoss: 0.49365609884262085\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4936383366584778\tF1-Score: 1.0\n",
            "54.\tLoss: 0.493623286485672\tF1-Score: 1.0\n",
            "55.\tLoss: 0.49361079931259155\tF1-Score: 1.0\n",
            "56.\tLoss: 0.49360042810440063\tF1-Score: 1.0\n",
            "57.\tLoss: 0.49359244108200073\tF1-Score: 1.0\n",
            "58.\tLoss: 0.4935861825942993\tF1-Score: 1.0\n",
            "59.\tLoss: 0.4935818910598755\tF1-Score: 1.0\n",
            "60.\tLoss: 0.493579238653183\tF1-Score: 1.0\n",
            "61.\tLoss: 0.493578165769577\tF1-Score: 1.0\n",
            "62.\tLoss: 0.49357855319976807\tF1-Score: 1.0\n",
            "63.\tLoss: 0.4935802221298218\tF1-Score: 1.0\n",
            "64.\tLoss: 0.4935832619667053\tF1-Score: 1.0\n",
            "65.\tLoss: 0.49358734488487244\tF1-Score: 1.0\n",
            "66.\tLoss: 0.49359261989593506\tF1-Score: 1.0\n",
            "67.\tLoss: 0.4935988783836365\tF1-Score: 1.0\n",
            "68.\tLoss: 0.49360591173171997\tF1-Score: 1.0\n",
            "69.\tLoss: 0.4936138987541199\tF1-Score: 1.0\n",
            "70.\tLoss: 0.4936228096485138\tF1-Score: 1.0\n",
            "71.\tLoss: 0.4936322569847107\tF1-Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.8819653391838074\tF1-Score: 0.0\n",
            "2.\tLoss: 1.0746209621429443\tF1-Score: 0.0\n",
            "3.\tLoss: 1.101270318031311\tF1-Score: 0.0\n",
            "4.\tLoss: 1.099574089050293\tF1-Score: 0.0\n",
            "5.\tLoss: 1.1000518798828125\tF1-Score: 0.0\n",
            "6.\tLoss: 1.1026333570480347\tF1-Score: 0.0\n",
            "7.\tLoss: 1.1006648540496826\tF1-Score: 0.0\n",
            "8.\tLoss: 1.0985887050628662\tF1-Score: 0.0\n",
            "9.\tLoss: 1.0971416234970093\tF1-Score: 0.0\n",
            "10.\tLoss: 1.0961401462554932\tF1-Score: 0.0\n",
            "11.\tLoss: 1.095442771911621\tF1-Score: 0.0\n",
            "12.\tLoss: 1.0949382781982422\tF1-Score: 0.0\n",
            "13.\tLoss: 1.094617486000061\tF1-Score: 0.0\n",
            "14.\tLoss: 1.0943593978881836\tF1-Score: 0.0\n",
            "15.\tLoss: 1.0948278903961182\tF1-Score: 0.0\n",
            "16.\tLoss: 1.0945258140563965\tF1-Score: 0.0\n",
            "17.\tLoss: 1.0944856405258179\tF1-Score: 0.0\n",
            "18.\tLoss: 1.0945732593536377\tF1-Score: 0.0\n",
            "19.\tLoss: 1.0947494506835938\tF1-Score: 0.0\n",
            "20.\tLoss: 1.0949630737304688\tF1-Score: 0.0\n",
            "21.\tLoss: 1.0952180624008179\tF1-Score: 0.0\n",
            "22.\tLoss: 1.095518708229065\tF1-Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.4395131468772888\tF1-Score: 1.0\n",
            "2.\tLoss: 0.5323485136032104\tF1-Score: 1.0\n",
            "3.\tLoss: 0.5437688231468201\tF1-Score: 1.0\n",
            "4.\tLoss: 0.5382479429244995\tF1-Score: 1.0\n",
            "5.\tLoss: 0.5312632322311401\tF1-Score: 1.0\n",
            "6.\tLoss: 0.5287757515907288\tF1-Score: 1.0\n",
            "7.\tLoss: 0.5272478461265564\tF1-Score: 1.0\n",
            "8.\tLoss: 0.526122510433197\tF1-Score: 1.0\n",
            "9.\tLoss: 0.5248066782951355\tF1-Score: 1.0\n",
            "10.\tLoss: 0.5237318277359009\tF1-Score: 1.0\n",
            "11.\tLoss: 0.5224337577819824\tF1-Score: 1.0\n",
            "12.\tLoss: 0.5205456018447876\tF1-Score: 1.0\n",
            "13.\tLoss: 0.5189662575721741\tF1-Score: 1.0\n",
            "14.\tLoss: 0.5176694989204407\tF1-Score: 1.0\n",
            "15.\tLoss: 0.517448902130127\tF1-Score: 1.0\n",
            "16.\tLoss: 0.5147578716278076\tF1-Score: 1.0\n",
            "17.\tLoss: 0.5147565007209778\tF1-Score: 1.0\n",
            "18.\tLoss: 0.5121819376945496\tF1-Score: 1.0\n",
            "19.\tLoss: 0.5123520493507385\tF1-Score: 1.0\n",
            "20.\tLoss: 0.5116482377052307\tF1-Score: 1.0\n",
            "21.\tLoss: 0.5092971920967102\tF1-Score: 1.0\n",
            "22.\tLoss: 0.5085961818695068\tF1-Score: 1.0\n",
            "23.\tLoss: 0.5067105889320374\tF1-Score: 1.0\n",
            "24.\tLoss: 0.5061752200126648\tF1-Score: 1.0\n",
            "25.\tLoss: 0.5042070746421814\tF1-Score: 1.0\n",
            "26.\tLoss: 0.5036303997039795\tF1-Score: 1.0\n",
            "27.\tLoss: 0.5015320181846619\tF1-Score: 1.0\n",
            "28.\tLoss: 0.4996233880519867\tF1-Score: 1.0\n",
            "29.\tLoss: 0.49938833713531494\tF1-Score: 1.0\n",
            "30.\tLoss: 0.4973354637622833\tF1-Score: 1.0\n",
            "31.\tLoss: 0.4952515959739685\tF1-Score: 1.0\n",
            "32.\tLoss: 0.4950180649757385\tF1-Score: 1.0\n",
            "33.\tLoss: 0.49298742413520813\tF1-Score: 1.0\n",
            "34.\tLoss: 0.4932023286819458\tF1-Score: 1.0\n",
            "35.\tLoss: 0.490784227848053\tF1-Score: 1.0\n",
            "36.\tLoss: 0.4886084496974945\tF1-Score: 1.0\n",
            "37.\tLoss: 0.4862600564956665\tF1-Score: 1.0\n",
            "38.\tLoss: 0.4844381809234619\tF1-Score: 1.0\n",
            "39.\tLoss: 0.4822865426540375\tF1-Score: 1.0\n",
            "40.\tLoss: 0.48097920417785645\tF1-Score: 1.0\n",
            "41.\tLoss: 0.47905680537223816\tF1-Score: 1.0\n",
            "42.\tLoss: 0.4765377640724182\tF1-Score: 1.0\n",
            "43.\tLoss: 0.47465845942497253\tF1-Score: 1.0\n",
            "44.\tLoss: 0.4728964865207672\tF1-Score: 1.0\n",
            "45.\tLoss: 0.4704899191856384\tF1-Score: 1.0\n",
            "46.\tLoss: 0.46879005432128906\tF1-Score: 1.0\n",
            "47.\tLoss: 0.4665996730327606\tF1-Score: 1.0\n",
            "48.\tLoss: 0.4648563861846924\tF1-Score: 1.0\n",
            "49.\tLoss: 0.46270251274108887\tF1-Score: 1.0\n",
            "50.\tLoss: 0.46499529480934143\tF1-Score: 1.0\n",
            "51.\tLoss: 0.46229416131973267\tF1-Score: 1.0\n",
            "52.\tLoss: 0.43598324060440063\tF1-Score: 1.0\n",
            "53.\tLoss: 0.4460362493991852\tF1-Score: 1.0\n",
            "54.\tLoss: 0.4551794230937958\tF1-Score: 1.0\n",
            "55.\tLoss: 0.43026939034461975\tF1-Score: 1.0\n",
            "56.\tLoss: 0.4462818205356598\tF1-Score: 1.0\n",
            "57.\tLoss: 0.4234825074672699\tF1-Score: 1.0\n",
            "58.\tLoss: 0.42631983757019043\tF1-Score: 1.0\n",
            "59.\tLoss: 0.42941784858703613\tF1-Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Data vectorized correctly\n",
            "1.\tLoss: 0.7620685696601868\tF1-Score: 0.0\n",
            "2.\tLoss: 0.6323961019515991\tF1-Score: 1.0\n",
            "3.\tLoss: 0.527485728263855\tF1-Score: 1.0\n",
            "4.\tLoss: 0.500312328338623\tF1-Score: 1.0\n",
            "5.\tLoss: 0.484281063079834\tF1-Score: 1.0\n",
            "6.\tLoss: 0.491383820772171\tF1-Score: 1.0\n",
            "7.\tLoss: 0.4945339560508728\tF1-Score: 1.0\n"
          ]
        }
      ]
    }
  ]
}